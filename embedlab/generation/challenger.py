"""Challenger agent that generates increasingly difficult queries."""
from __future__ import annotations
import json
import random
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import numpy as np

from ..core.llm_client import LLMClient, LLMConfig
from ..core.embedding_client import EmbeddingClient
from ..config.evolution_config import ChallengerState, EvolutionConfig
from ..config.task_goals import TaskType
from ..hierarchy import HierNode

@dataclass
class ChallengeQuery:
    """A query generated by the Challenger."""
    text: str
    target_path: str
    difficulty: float
    ambiguity: float
    reasoning: str

class ChallengerAgent:
    """
    Generates increasingly difficult queries for hierarchy routing.
    Inspired by R-Zero's Challenger that proposes tasks near Solver's capability edge.
    """

    def __init__(
        self,
        llm_client: LLMClient,
        embedding_client: EmbeddingClient,
        config: EvolutionConfig,
        hierarchy_root: HierNode
    ):
        self.llm = llm_client
        self.embedder = embedding_client
        self.config = config
        self.hierarchy = hierarchy_root
        self.state = ChallengerState(current_difficulty=config.initial_difficulty)

        # Build node mapping
        self.nodes_by_path = self._build_node_map(hierarchy_root)
        self.leaf_paths = [path for path, node in self.nodes_by_path.items() if node.is_leaf()]

    def _build_node_map(self, root: HierNode) -> Dict[str, HierNode]:
        """Build path -> node mapping."""
        nodes = {}
        for node in root.iter_nodes():
            nodes[node.path] = node
        return nodes

    def generate_batch(
        self,
        batch_size: int,
        solver_performance: Optional[Dict[str, float]] = None
    ) -> List[ChallengeQuery]:
        """
        Generate a batch of challenging queries.

        Args:
            batch_size: Number of queries to generate
            solver_performance: Recent Solver performance metrics

        Returns:
            List of challenge queries
        """
        queries = []

        # Adjust difficulty based on solver performance
        if solver_performance:
            self._adjust_difficulty(solver_performance)

        # Generate queries with different strategies
        strategies = [
            self._generate_boundary_query,
            self._generate_ambiguous_query,
            self._generate_misleading_query,
            self._generate_composite_query
        ]

        for _ in range(batch_size):
            strategy = random.choice(strategies)
            query = strategy()
            if query:
                queries.append(query)

        self.state.generated_count += len(queries)
        self.state.recent_queries = [q.text for q in queries[-10:]]  # Keep last 10

        return queries

    def _generate_boundary_query(self) -> Optional[ChallengeQuery]:
        """Generate query at boundary between two categories."""
        # Select two sibling nodes
        parent_path = random.choice([p for p in self.nodes_by_path if self.nodes_by_path[p].children])
        parent = self.nodes_by_path[parent_path]

        if len(parent.children) < 2:
            return None

        node1, node2 = random.sample(parent.children, 2)

        # Adapt prompt based on task goal
        task_goal = self.config.task_goal
        if task_goal and task_goal.type == TaskType.SEMANTIC_SEARCH:
            query_type = "search query"
            context = "that could retrieve documents from either"
        elif task_goal and task_goal.type == TaskType.QUESTION_ANSWERING:
            query_type = "question"
            context = "that could be answered by content from either"
        elif task_goal and task_goal.type == TaskType.INTENT_CLASSIFICATION:
            query_type = "user request"
            context = "that expresses intent related to either"
        else:
            query_type = "customer support query"
            context = "that could belong to either"

        prompt = f"""Generate a {query_type} {context} "{node1.name}" or "{node2.name}".

Category 1: {node1.name}
Description: {node1.description}

Category 2: {node2.name}
Description: {node2.description}

Create a query that has elements of both categories, making it genuinely difficult to classify.
The query should be realistic and something a real customer might ask.

Respond in JSON format:
{{
    "query": "the ambiguous query text",
    "primary_category": "path of the category it slightly leans toward",
    "ambiguity_score": 0.0-1.0 (how ambiguous it is),
    "reasoning": "why this is challenging"
}}"""

        response = self.llm.generate_json(prompt, temperature=self.config.challenger_temperature)

        if response and "query" in response:
            # Determine which leaf under the chosen node
            primary_node = node1 if node1.path in response.get("primary_category", "") else node2
            target_leaf = self._select_leaf_under_node(primary_node)

            return ChallengeQuery(
                text=response["query"],
                target_path=target_leaf,
                difficulty=self.state.current_difficulty,
                ambiguity=response.get("ambiguity_score", 0.5),
                reasoning=response.get("reasoning", "")
            )

        return None

    def _generate_ambiguous_query(self) -> Optional[ChallengeQuery]:
        """Generate inherently ambiguous query."""
        target_path = random.choice(self.leaf_paths)
        target_node = self.nodes_by_path[target_path]

        # Find similar nodes
        similar_nodes = self._find_similar_nodes(target_node)

        # Adapt based on task goal
        task_goal = self.config.task_goal
        if task_goal and task_goal.type == TaskType.SEMANTIC_SEARCH:
            query_type = "search query"
            objective = "find documents about"
        elif task_goal and task_goal.type == TaskType.QUESTION_ANSWERING:
            query_type = "question"
            objective = "get answers about"
        elif task_goal and task_goal.type == TaskType.INTENT_CLASSIFICATION:
            query_type = "user request"
            objective = "express intent for"
        else:
            query_type = "customer support query"
            objective = "get help with"

        prompt = f"""Generate an ambiguous {query_type} to {objective} "{target_node.name}".

Target category: {target_node.name}
Description: {target_node.description}

Similar categories that could confuse: {', '.join([n.name for n in similar_nodes[:3]])}

Create a query that:
1. Is technically about {target_node.name}
2. Uses vague or ambiguous language
3. Could be misinterpreted as belonging to similar categories
4. Represents a realistic scenario for the task goal: {task_goal.objective if task_goal else 'routing queries'}

Respond in JSON format:
{{
    "query": "the ambiguous query",
    "ambiguity_techniques": ["list of techniques used"],
    "potential_misclassifications": ["other categories it might match"]
}}"""

        response = self.llm.generate_json(prompt, temperature=self.config.challenger_temperature)

        if response and "query" in response:
            return ChallengeQuery(
                text=response["query"],
                target_path=target_path,
                difficulty=self.state.current_difficulty,
                ambiguity=self.config.ambiguity_target,
                reasoning=f"Ambiguous phrasing: {', '.join(response.get('ambiguity_techniques', []))}"
            )

        return None

    def _generate_misleading_query(self) -> Optional[ChallengeQuery]:
        """Generate query with misleading keywords."""
        target_path = random.choice(self.leaf_paths)
        target_node = self.nodes_by_path[target_path]

        # Choose a different node for misleading keywords
        other_paths = [p for p in self.leaf_paths if p != target_path]
        misleading_path = random.choice(other_paths)
        misleading_node = self.nodes_by_path[misleading_path]

        # Adapt based on task goal
        task_goal = self.config.task_goal
        if task_goal:
            query_context = f"for the task: {task_goal.objective}"
        else:
            query_context = "for routing"

        prompt = f"""Generate a query that belongs to "{target_node.name}" but contains misleading keywords from "{misleading_node.name}" {query_context}.

True category: {target_node.name}
Description: {target_node.description}

Misleading category: {misleading_node.name}
Description: {misleading_node.description}

Create a query that:
1. Has a core issue clearly in {target_node.name}
2. Uses terminology or keywords typically associated with {misleading_node.name}
3. Would trip up keyword-based routing
4. Represents a realistic customer scenario

Respond in JSON format:
{{
    "query": "the misleading query",
    "misleading_keywords": ["keywords from wrong category"],
    "core_issue": "what the query is actually about"
}}"""

        response = self.llm.generate_json(prompt, temperature=self.config.challenger_temperature)

        if response and "query" in response:
            return ChallengeQuery(
                text=response["query"],
                target_path=target_path,
                difficulty=self.state.current_difficulty * 1.2,  # Extra difficult
                ambiguity=0.6,
                reasoning=f"Misleading keywords: {', '.join(response.get('misleading_keywords', []))}"
            )

        return None

    def _generate_composite_query(self) -> Optional[ChallengeQuery]:
        """Generate query with multiple issues."""
        # Select 2-3 leaf nodes
        num_issues = random.randint(2, 3)
        selected_paths = random.sample(self.leaf_paths, min(num_issues, len(self.leaf_paths)))
        primary_path = selected_paths[0]
        secondary_paths = selected_paths[1:]

        nodes_desc = "\n".join([
            f"- {self.nodes_by_path[p].name}: {self.nodes_by_path[p].description}"
            for p in selected_paths
        ])

        # Adapt based on task goal
        task_goal = self.config.task_goal
        if task_goal and task_goal.type == TaskType.SEMANTIC_SEARCH:
            query_type = "search query covering multiple topics"
        elif task_goal and task_goal.type == TaskType.QUESTION_ANSWERING:
            query_type = "multi-part question"
        elif task_goal and task_goal.type == TaskType.INTENT_CLASSIFICATION:
            query_type = "request with multiple intents"
        else:
            query_type = "customer query with multiple issues"

        prompt = f"""Generate a complex {query_type}.

Primary issue category: {self.nodes_by_path[primary_path].name}
Secondary issues: {', '.join([self.nodes_by_path[p].name for p in secondary_paths])}

Categories involved:
{nodes_desc}

Create a query that:
1. Has a primary issue in {self.nodes_by_path[primary_path].name}
2. Also mentions problems from the secondary categories
3. Requires identifying the main concern vs secondary mentions
4. Represents a realistic scenario where a customer has multiple related issues

Respond in JSON format:
{{
    "query": "the composite query",
    "primary_issue": "main problem description",
    "secondary_issues": ["list of secondary problems"],
    "complexity_score": 0.0-1.0
}}"""

        response = self.llm.generate_json(prompt, temperature=self.config.challenger_temperature)

        if response and "query" in response:
            return ChallengeQuery(
                text=response["query"],
                target_path=primary_path,
                difficulty=self.state.current_difficulty * 1.1,
                ambiguity=response.get("complexity_score", 0.7),
                reasoning=f"Composite query with {len(secondary_paths)} secondary issues"
            )

        return None

    def _find_similar_nodes(self, target_node: HierNode) -> List[HierNode]:
        """Find nodes similar to target (for confusion)."""
        # Get all other leaf nodes
        other_nodes = [
            self.nodes_by_path[p] for p in self.leaf_paths
            if p != target_node.path
        ]

        # Sort by description similarity (simple keyword overlap)
        target_words = set(target_node.description.lower().split())
        similarities = []

        for node in other_nodes:
            node_words = set(node.description.lower().split())
            overlap = len(target_words & node_words)
            similarities.append((node, overlap))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return [node for node, _ in similarities]

    def _select_leaf_under_node(self, node: HierNode) -> str:
        """Select a leaf path under given node."""
        if node.is_leaf():
            return node.path

        # Recursively find all leaves
        leaves = []
        def find_leaves(n):
            if n.is_leaf():
                leaves.append(n.path)
            else:
                for child in n.children:
                    find_leaves(child)

        find_leaves(node)
        return random.choice(leaves) if leaves else node.path

    def _adjust_difficulty(self, solver_performance: Dict[str, float]):
        """Adjust difficulty based on Solver's performance."""
        accuracy = solver_performance.get("accuracy", 0.5)

        # If Solver is doing well, increase difficulty
        if accuracy > 0.8:
            self.state.current_difficulty = min(
                self.config.max_difficulty,
                self.state.current_difficulty + self.config.difficulty_increment
            )
        # If Solver is struggling, slightly decrease
        elif accuracy < 0.4:
            self.state.current_difficulty = max(
                self.config.initial_difficulty,
                self.state.current_difficulty - self.config.difficulty_increment * 0.5
            )

        self.state.success_rate = accuracy

    def calculate_reward(self, solver_results: Dict[str, float]) -> float:
        """
        Calculate Challenger's reward based on Solver's performance.
        Reward is high when queries are appropriately difficult.
        """
        accuracy = solver_results.get("accuracy", 0.0)

        # Optimal difficulty: Solver gets 60-70% correct
        optimal_accuracy = 0.65
        accuracy_penalty = abs(accuracy - optimal_accuracy)

        # Reward for appropriate difficulty
        difficulty_reward = 1.0 - accuracy_penalty

        # Bonus for generating diverse queries
        diversity_bonus = min(0.2, len(set(self.state.recent_queries)) / 10 * 0.2)

        reward = difficulty_reward + diversity_bonus
        return max(0.0, min(1.0, reward))
