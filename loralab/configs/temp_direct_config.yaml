challenger:
  cpu_moe_layers: 20
  executable: C:\Users\Jake\llama.cpp\build\bin\Release\llama-server.exe
  gpu_layers: 20
  model_path: C:\models\Qwen3-30B-A3B-Thinking-2507\Qwen3-30B-A3B-Thinking-2507-Q8_0.gguf
  port: 8080
  type: llama_cpp
  use_direct: true
evolution:
  bootstrap_size: 10
  dataset_size_per_gen: 10
  eval_ratio: 0.2
  generations: 50
  population_size: 10
output_dir: experiments/doc_gen_v1
solver:
  device: cuda
  lora_config:
    alpha: 32
    dropout: 0.1
    rank: 16
    target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj
  model_name: Qwen/Qwen3-4B-Instruct-2507
  quantization:
    enabled: true
    load_in_4bit: false
    load_in_8bit: true
  type: transformers
task:
  curriculum:
    initial_difficulty: 0.3
    target_success_rate: 0.7
  difficulty_progression: adaptive
  templates:
    code_documentation: 'Generate a complex Python function that requires detailed
      documentation.

      Difficulty level: {difficulty:.1f}


      The function should:

      - Have clear purpose but complex implementation

      - Include edge cases and error handling

      - Use type hints and advanced Python features

      - {difficulty_hints}


      Output only the function code:

      ```python

      '
  type: code_documentation
training:
  batch_size: 8
  clip_ratio: 0.2
  kl_penalty: 0.1
  learning_rate: 1e-5
  max_grad_norm: 1.0
  num_rollouts: 4
