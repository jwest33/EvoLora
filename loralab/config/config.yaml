# LoRALab Self-Supervised Evolution Configuration
# Evolutionary optimization for finding optimal LoRA adapters

mode: "self_supervised"

# ============================================================================
# SELF-SUPERVISED EVOLUTION CONFIGURATION
# ============================================================================

# Single model configuration (no teacher needed)
model:
  path: "Qwen/Qwen3-4B-Instruct-2507"  # HuggingFace ID or local path
  type: "transformers"
  torch_dtype: "float16"
  device_map: "auto"
  trust_remote_code: true
  low_cpu_mem_usage: true

# Evolution parameters
evolution:
  population_size: 6        # Number of variants per generation
  generations: 10           # Number of evolution cycles
  keep_top: 2              # Number of best variants to keep
  mutation_rate: 0.3       # Probability of mutating parameters
  crossover_rate: 0.2      # Proportion of offspring from crossover

# LoRA hyperparameter search space
lora_search_space:
  rank: [16, 32, 64, 128, 256]           # LoRA rank options
  alpha_multiplier: [1, 2, 3]            # alpha = rank * multiplier
  dropout: [0.05, 0.1, 0.15]             # Dropout rates
  learning_rate: [1e-5, 2e-5, 5e-5, 1e-4, 2e-4]  # Learning rates
  target_modules:                         # Modules to apply LoRA to
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

# Training parameters
training:
  batch_size: 4                    # Batch size per variant
  gradient_accumulation_steps: 16   # Effective batch = 64
  epochs_per_variant: 1            # Quick iterations for evolution
  max_grad_norm: 1.0               # Gradient clipping
  weight_decay: 0.01               # Weight decay
  warmup_ratio: 0.1                # Warmup proportion
  fp16: true                       # Use mixed precision
  gradient_checkpointing: false    # Memory vs speed tradeoff

# Dataset configuration
dataset:
  sources:                         # Datasets to use
    - "mmlu-pro"                   # Multi-task benchmark
  train_size: 10000                # Training examples
  eval_size: 1000                  # Evaluation examples
  seed: 42                         # Random seed for splits

# Output configuration
output_dir: "evolved_adapters"

# Logging
logging:
  level: "INFO"
  save_history: true
  track_metrics: ["accuracy", "perplexity", "fitness_score"]

# Quick test mode (for development)
quick_test:
  enabled: false
  population_size: 2
  generations: 2
  train_size: 100
  eval_size: 20
  