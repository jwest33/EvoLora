# LoRALab Gemma3-270M Evolution Configuration
# Supports both SFT and GRPO training methods

mode: "self_supervised"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

model:
  path: "unsloth/gemma-3-270m-it"      # Small Gemma3 model (270M params)
  backend: "unsloth"                    # Use Unsloth for optimization
  type: "transformers"
  torch_dtype: "float16"
  device_map: "cuda:0"
  trust_remote_code: true
  low_cpu_mem_usage: true
  # Unsloth-specific settings
  quantization: "none"                  # No quantization for 270M model
  max_seq_length: 2048                  # Gemma3 supports up to 8k context
  chat_template: "gemma3"               # Gemma3 chat template
  load_in_4bit: false                   # Disabled for small model
  load_in_8bit: false                   # Disabled for small model
  use_cache: false                      # Disable KV cache during training

# ============================================================================
# EVOLUTION PARAMETERS
# ============================================================================

evolution:
  population_size: 5
  generations: 10
  keep_top: 2
  mutation_rate: 0.35                   # Higher mutation for exploration
  crossover_rate: 0.25                  # Moderate crossover
  skip_perplexity: false                # Use full evaluation for small model

# ============================================================================
# LORA SEARCH SPACE
# ============================================================================

lora_search_space:
  rank: [16,32, 64, 128, 256]                   # Higher ranks for small model
  alpha_multiplier: [0.5, 1, 1.5, 2]            # Alpha = rank * multiplier
  dropout: [0.0]                                # Zero dropout for Unsloth
  learning_rate: [1e-4, 2e-4, 3e-4, 4e-4, 5e-4] # Higher LR for small model
  # Training hyperparameters
  weight_decay: [0.0, 0.01, 0.05]               # Regularization options
  warmup_ratio: [0.0, 0.1, 0.2]                 # Warmup ratio options
  max_grad_norm: [0.5, 1.0, 2.0]                # Gradient clipping options
  # LoRA-specific options
  use_rslora: [false, true]                     # Rank-Stabilized LoRA
  target_modules_preset: ["standard", "extended", "full"]  # Module combinations
  use_gradient_checkpointing: true              # Unsloth's optimized checkpointing

# ============================================================================
# TRAINING PARAMETERS
# ============================================================================

training:
  method: "sft"                         # Default to SFT (overridden by --grpo flag)
  batch_size: 10                        # Larger batch for small model
  gradient_accumulation_steps: 2        # Effective batch = 16
  epochs_per_variant: 1                 # Single epoch per variant
  max_grad_norm: 1.0                    # Gradient clipping
  weight_decay: 0.01                    # Weight decay
  warmup_ratio: 0.1                     # Warmup proportion
  fp16: true                            # Use FP16 for speed
  bf16: false                           # Avoid BFloat16 issues
  gradient_checkpointing: false         # Let Unsloth handle this
  dataloader_num_workers: 0             # Windows compatibility
  dataloader_pin_memory: true           # Pin memory
  dataloader_prefetch_factor: 2         # Prefetch batches

# ============================================================================
# GRPO CONFIGURATION (Used when --grpo flag is set)
# ============================================================================

grpo:
  enabled: false                        # Enabled via command line flag
  max_steps: 10                         # Training steps for GRPO
  temperature: 0.8                      # Generation temperature
  max_prompt_length: 512                # Max prompt length
  max_completion_length: 512            # Max completion length
  # Reward function weights
  reward_weights:
    format: 1.0                         # Format compliance weight
    accuracy: 3.0                       # Higher weight for accuracy
    reasoning: 2.0                      # Reasoning quality weight
  # Pre-training options
  pre_train_format: true                # Pre-train on format
  format_examples: 10                   # Format examples for small model

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

dataset:
  sources:
    - "gsm8k"
  train_size: 1000
  eval_size: 10
  seed: 42
  max_seq_length: 512
  use_cache: true
  cache_dir: "cache"

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================

output:
  base_dir: "lora_runs_gemma"           # Output directory for Gemma
  run_name: null                        # Auto-generate name
  keep_runs: 5                          # Keep recent runs

# ============================================================================
# LOGGING
# ============================================================================

logging:
  level: "INFO"
  save_history: true
  track_metrics: ["accuracy", "perplexity", "fitness_score", "rewards"]

# ============================================================================
# QUICK TEST MODE
# ============================================================================

quick_test:
  enabled: false
  population_size: 2
  generations: 2
  train_size: 100
  eval_size: 20
