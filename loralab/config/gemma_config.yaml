# LoRALab Gemma3-270M Evolution Configuration
# Optimized for small model (270M) based on research best practices
# Supports both SFT and GRPO training methods

mode: "self_supervised"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

model:
  path: "unsloth/gemma-3-270m-it"      # Small Gemma3 model (270M params)
  backend: "unsloth"                    # Use Unsloth for optimization
  type: "transformers"
  torch_dtype: "float16"
  device_map: "cuda:0"
  trust_remote_code: true
  low_cpu_mem_usage: true
  # Unsloth-specific settings
  quantization: "none"                  # No quantization for 270M model
  max_seq_length: 2048                  # Gemma3 supports up to 8k context
  chat_template: "gemma3"               # Gemma3 chat template
  load_in_4bit: false                   # Disabled for small model
  load_in_8bit: false                   # Disabled for small model
  use_cache: false                      # Disable KV cache during training

# ============================================================================
# EVOLUTION PARAMETERS
# ============================================================================

evolution:
  population_size: 12                   # More variants to explore search space
  generations: 15                       # More generations for better convergence
  keep_top: 3                           # Keep more top performers
  mutation_rate: 0.4                    # Higher mutation for exploration
  crossover_rate: 0.3                   # Increased crossover for diversity
  skip_perplexity: false                # Use full evaluation for small model

# ============================================================================
# LORA SEARCH SPACE - Optimized for 270M model based on research
# ============================================================================

lora_search_space:
  # Research recommends rank 4-8 for small models, with some higher options
  rank: [4, 8, 16, 32]                  # Lower ranks to prevent overfitting
  alpha_multiplier: [1, 2]              # Alpha = rank * multiplier (research: 2x is good heuristic)
  dropout: [0.0, 0.1]                   # Light dropout for regularization
  learning_rate: [5e-5, 1e-4, 2e-4]    # Conservative LRs for stability
  # Training hyperparameters - stronger regularization for small model
  weight_decay: [0.01, 0.05, 0.1]      # Stronger regularization to prevent overfitting
  warmup_ratio: [0.05, 0.1, 0.15]      # Warmup helps stability
  max_grad_norm: [0.5, 1.0]             # Tighter gradient clipping
  # LoRA-specific options
  use_rslora: [false, true]             # Rank-Stabilized LoRA can help small models
  target_modules_preset: ["standard", "extended"]  # Avoid "full" for small model
  use_gradient_checkpointing: true      # Unsloth's optimized checkpointing

# ============================================================================
# TRAINING PARAMETERS - Optimized to prevent overfitting
# ============================================================================

training:
  method: "sft"                         # Default to SFT (overridden by --grpo flag)
  batch_size: 16                        # Moderate batch size for better gradients
  gradient_accumulation_steps: 2        # Effective batch = 32
  epochs_per_variant: 2                 # 1-2 epochs max (research shows more causes overfitting)
  max_grad_norm: 1.0                    # Gradient clipping
  weight_decay: 0.05                    # Strong default regularization
  warmup_ratio: 0.1                     # Warmup proportion
  fp16: true                            # Use FP16 for speed
  bf16: false                           # Avoid BFloat16 issues
  gradient_checkpointing: false         # Let Unsloth handle this
  dataloader_num_workers: 0             # Windows compatibility
  dataloader_pin_memory: true           # Pin memory
  dataloader_prefetch_factor: 2         # Prefetch batches
  # Early stopping to prevent overfitting
  eval_steps: 50                         # Evaluate frequently
  save_steps: 50                         # Save checkpoints frequently
  early_stopping_patience: 3             # Stop if no improvement for 3 evals

# ============================================================================
# GRPO CONFIGURATION (Used when --grpo flag is set)
# ============================================================================

grpo:
  enabled: false                        # Enabled via command line flag
  max_steps: 20                         # Very few steps to avoid instability
  num_generations: 1                    # Completions per prompt (not evolution generations!)
  temperature: 0.6                      # Lower temperature for stability
  max_prompt_length: 256                # Shorter prompts for small model
  max_completion_length: 256            # Shorter completions for stability
  # KL penalty to prevent divergence
  kl_penalty: "kl"                      # Use KL penalty
  init_kl_coef: 0.2                     # Initial KL coefficient
  # Generation parameters for stability
  top_k: 50                             # Limit sampling
  top_p: 0.9                            # Nucleus sampling
  do_sample: true                       # Enable sampling
  # Reward function weights
  reward_weights:
    format: 1.0                         # Format compliance weight
    accuracy: 2.0                       # Moderate accuracy weight
    reasoning: 1.5                      # Reasoning quality weight
  # Pre-training options
  pre_train_format: true                # Pre-train on format
  format_examples: 5                    # Very few examples for small model
  pre_train_epochs: 1                   # Single epoch for format pre-training

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

dataset:
  sources:
    - "gsm8k"
  train_size: 2000                      # Larger dataset to reduce overfitting
  eval_size: 100                        # Larger eval set for better metrics
  seed: 42
  max_seq_length: 512
  use_cache: true
  cache_dir: "cache"

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================

output:
  base_dir: "lora_runs_gemma"           # Output directory for Gemma
  run_name: null                        # Auto-generate name
  keep_runs: 5                          # Keep recent runs

# ============================================================================
# LOGGING
# ============================================================================

logging:
  level: "INFO"
  save_history: true
  track_metrics: ["accuracy", "perplexity", "fitness_score", "rewards"]

# ============================================================================
# QUICK TEST MODE
# ============================================================================

quick_test:
  enabled: false
  population_size: 3
  generations: 3
  train_size: 200
  eval_size: 20