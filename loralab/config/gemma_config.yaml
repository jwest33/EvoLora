# LoRALab Gemma3-270M Evolution Configuration
# Optimized for small model (270M) based on research best practices
# Supports both SFT and GRPO training methods

mode: "self_supervised"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

model:
  path: "unsloth/gemma-3-270m-it"      # Small Gemma3 model (270M params)
  backend: "unsloth"                    # Use Unsloth for optimization
  type: "transformers"
  torch_dtype: "float16"
  device_map: "cuda:0"
  trust_remote_code: true
  low_cpu_mem_usage: true
  # Unsloth-specific settings
  quantization: "none"                  # No quantization for 270M model
  max_seq_length: 2048                  # Gemma3 supports up to 8k context
  chat_template: "gemma3"               # Gemma3 chat template
  load_in_4bit: false                   # Disabled for small model
  load_in_8bit: false                   # Disabled for small model
  use_cache: false                      # Disable KV cache during training

# ============================================================================
# EVOLUTION PARAMETERS
# ============================================================================

evolution:
  population_size: 4                   # More variants to explore search space
  generations: 10                       # More generations for better convergence
  keep_top: 2                           # Keep more top performers
  mutation_rate: 0.4                    # Higher mutation for exploration
  crossover_rate: 0.3                   # Increased crossover for diversity
  skip_perplexity: false                # Use full evaluation for small model

# ============================================================================
# LORA SEARCH SPACE - Optimized for 270M model based on research
# ============================================================================

lora_search_space:
  # Research recommends rank 4-8 for small models, with some higher options
  rank: [4, 8, 16]                      # Lower ranks for small model (removed 32)
  alpha_multiplier: [1, 2]              # Alpha = rank * multiplier (research: 2x is good heuristic)
  dropout: [0.05, 0.1]                  # Light dropout for regularization
  learning_rate: [1e-5, 2e-5, 5e-5]    # Much lower LRs for GRPO stability
  # Training hyperparameters - stronger regularization for small model
  weight_decay: [0.01, 0.05, 0.1]      # Stronger regularization to prevent overfitting
  warmup_ratio: [0.05, 0.1, 0.15]      # Warmup helps stability
  max_grad_norm: [0.5, 1.0]             # Tighter gradient clipping
  # LoRA-specific options
  use_rslora: [false, true]             # Rank-Stabilized LoRA can help small models
  target_modules_preset: ["standard", "extended"]  # Avoid "full" for small model
  use_gradient_checkpointing: true      # Unsloth's optimized checkpointing

# ============================================================================
# TRAINING PARAMETERS - Optimized to prevent overfitting
# ============================================================================

training:
  method: "sft"                         # Default to SFT (overridden by --grpo flag)
  batch_size: 16                        # Moderate batch size for better gradients
  gradient_accumulation_steps: 2        # Effective batch = 32
  epochs_per_variant: 2                 # 1-2 epochs max (research shows more causes overfitting)
  max_grad_norm: 1.0                    # Gradient clipping
  weight_decay: 0.05                    # Strong default regularization
  warmup_ratio: 0.1                     # Warmup proportion
  fp16: true                            # Use FP16 for speed
  bf16: false                           # Avoid BFloat16 issues
  gradient_checkpointing: false         # Let Unsloth handle this
  dataloader_num_workers: 0             # Windows compatibility
  dataloader_pin_memory: true           # Pin memory
  dataloader_prefetch_factor: 2         # Prefetch batches
  # Early stopping to prevent overfitting
  eval_steps: 10                         # Evaluate frequently
  save_steps: 10                         # Save checkpoints frequently
  early_stopping_patience: 3             # Stop if no improvement for 3 evals

# ============================================================================
# GRPO CONFIGURATION (Used when --grpo flag is set)
# ============================================================================

grpo:
  enabled: false                        # Enabled via command line flag
  max_steps: 25                         # Very few steps to avoid instability
  num_generations: 2                    # Minimum 2 for GRPO (3 can be unstable)
  temperature: 0.5                      # Lower temperature for more stability
  max_prompt_length: 256                # Shorter for stability
  max_completion_length: 128            # Much shorter to prevent divergence
  # KL regularization - CRITICAL for stability
  beta: 0.5                             # Higher KL penalty to prevent divergence
  epsilon: 0.1                          # Smaller clipping for stability
  # Generation parameters for stability
  top_k: 50                             # Limit sampling
  top_p: 0.9                            # Nucleus sampling
  # Loss configuration
  # loss_type: "dapo"                   # DAPO has bugs in Unsloth, use default
  # scale_rewards: "group"              # Scale by group std
  # Reward function weights
  reward_weights:
    format: 1.0                         # Format compliance weight
    accuracy: 2.0                       # Moderate accuracy weight
    reasoning: 1.5                      # Reasoning quality weight
  # Pre-training options
  pre_train_format: true                # Pre-train on format
  format_examples: 5                    # Very few examples for small model
  pre_train_epochs: 1                   # Single epoch for format pre-training

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

dataset:
  sources:
    - "simple-math"                     # Much simpler arithmetic problems
  train_size: 1000                     # Reduced for GRPO stability testing
  eval_size: 100                       # Smaller eval for faster testing
  seed: 42
  max_seq_length: 512
  use_cache: true
  cache_dir: "cache"

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================

output:
  base_dir: "lora_runs_gemma"           # Output directory for Gemma
  run_name: null                        # Auto-generate name
  keep_runs: 5                          # Keep recent runs

# ============================================================================
# LOGGING
# ============================================================================

logging:
  level: "INFO"
  save_history: true
  track_metrics: ["accuracy", "perplexity", "fitness_score", "rewards"]

# ============================================================================
# QUICK TEST MODE
# ============================================================================

quick_test:
  enabled: false
  population_size: 3
  generations: 3
  train_size: 200
  eval_size: 20
