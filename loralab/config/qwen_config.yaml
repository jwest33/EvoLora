# LoRALab Qwen3-4B Evolution Configuration
# Optimized for 4B model based on research best practices
# Supports both SFT and GRPO training methods

mode: "self_supervised"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

model:
  path: "Qwen/Qwen3-4B-Instruct-2507"  # HuggingFace model ID
  backend: "unsloth"                    # Use Unsloth for optimization
  type: "transformers"
  torch_dtype: "float16"
  device_map: "cuda:0"
  trust_remote_code: true
  low_cpu_mem_usage: true
  # Unsloth-specific settings
  quantization: "4bit"                  # 4-bit quantization for memory efficiency
  max_seq_length: 2048                  # Maximum sequence length
  chat_template: "qwen3-instruct"       # Qwen3 chat template
  load_in_4bit: true                    # Enable 4-bit loading
  use_cache: false                      # Disable KV cache during training

# ============================================================================
# EVOLUTION PARAMETERS
# ============================================================================

evolution:
  population_size: 10                   # More variants to explore search space
  generations: 12                       # Sufficient generations for convergence
  keep_top: 3                           # Keep more top performers
  mutation_rate: 0.35                   # Balanced mutation rate
  crossover_rate: 0.25                  # Crossover probability
  skip_perplexity: false                # Use full perplexity evaluation

# ============================================================================
# LORA SEARCH SPACE - Optimized for 4B model
# ============================================================================

lora_search_space:
  # Research shows rank 8-32 works well for medium models
  rank: [8, 16, 32, 64]                 # Balanced rank options for 4B model
  alpha_multiplier: [1, 2]              # Alpha = rank * multiplier
  dropout: [0.0, 0.05]                  # Light dropout for regularization
  learning_rate: [2e-5, 5e-5, 1e-4]    # Conservative learning rates
  # Training hyperparameters
  weight_decay: [0.0, 0.01, 0.02]      # Moderate regularization
  warmup_ratio: [0.05, 0.1]             # Warmup ratio options
  max_grad_norm: [0.5, 1.0]             # Gradient clipping options
  # LoRA-specific options
  use_rslora: [false, true]             # Rank-Stabilized LoRA
  target_modules_preset: ["standard", "extended"]  # Module combinations
  use_gradient_checkpointing: true      # Unsloth's optimized checkpointing

# ============================================================================
# TRAINING PARAMETERS - Optimized for 4B model
# ============================================================================

training:
  method: "sft"                         # Default to SFT (overridden by --grpo flag)
  batch_size: 4                         # Conservative batch size for 4-bit model
  gradient_accumulation_steps: 4        # Effective batch = 16
  epochs_per_variant: 3                 # 2-3 epochs for medium models
  max_grad_norm: 1.0                    # Gradient clipping
  weight_decay: 0.01                    # Default weight decay
  warmup_ratio: 0.1                     # Warmup proportion
  fp16: true                            # Use FP16 training
  bf16: false                           # Disable BF16
  gradient_checkpointing: false         # Let Unsloth handle this
  dataloader_num_workers: 0             # Windows compatibility
  dataloader_pin_memory: true           # Pin memory
  dataloader_prefetch_factor: 2         # Prefetch batches
  # Evaluation settings
  eval_steps: 100                       # Evaluate periodically
  save_steps: 100                       # Save checkpoints
  early_stopping_patience: 3            # Stop if no improvement

# ============================================================================
# GRPO CONFIGURATION (Used when --grpo flag is set)
# ============================================================================

grpo:
  enabled: false                        # Enabled via command line flag
  max_steps: 50                         # Training steps for GRPO
  num_generations: 3                    # Minimum 2 required for GRPO advantage calculation
  temperature: 0.7                      # Generation temperature
  max_prompt_length: 384                # Max prompt length
  max_completion_length: 384            # Max completion length
  # KL regularization
  beta: 0.05                            # Lower KL for larger model
  epsilon: 0.2                          # Clipping epsilon
  # Generation parameters
  top_k: 50                             # Limit sampling
  top_p: 0.95                           # Nucleus sampling
  # Loss configuration
  # loss_type: "dapo"                   # DAPO has bugs in Unsloth, use default
  # scale_rewards: "group"              # Scale by group std
  # Reward function weights
  reward_weights:
    format: 1.0                         # Format compliance weight
    accuracy: 2.5                       # Higher accuracy weight
    reasoning: 1.5                      # Reasoning quality weight
  # Pre-training options
  pre_train_format: true                # Pre-train on format
  format_examples: 15                   # Format examples for 4B model
  pre_train_epochs: 2                   # Pre-training epochs

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================

dataset:
  sources:                              # Datasets to use
    - "gsm8k"                           # Math reasoning dataset
  train_size: 3000                      # Larger training set for 4B model
  eval_size: 200                        # Evaluation samples
  seed: 42                              # Random seed
  max_seq_length: 512                   # Sequence length
  use_cache: true                       # Enable caching
  cache_dir: "cache"                    # Cache directory

# ============================================================================
# OUTPUT CONFIGURATION
# ============================================================================

output:
  base_dir: "lora_runs_qwen"            # Output directory for Qwen
  run_name: null                        # Auto-generate name
  keep_runs: 5                          # Keep recent runs

# ============================================================================
# LOGGING
# ============================================================================

logging:
  level: "INFO"
  save_history: true
  track_metrics: ["accuracy", "perplexity", "fitness_score", "rewards"]

# ============================================================================
# QUICK TEST MODE
# ============================================================================

quick_test:
  enabled: false
  population_size: 3
  generations: 3
  train_size: 300
  eval_size: 30
