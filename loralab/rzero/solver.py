"""Solver Agent for R-Zero Framework

The Solver learns to solve increasingly difficult problems generated by the Challenger,
using GRPO with verifiable rewards based on answer correctness.
"""

import logging
import re
import torch
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from collections import Counter

logger = logging.getLogger(__name__)


@dataclass
class SolverDataPoint:
    """Container for a training example for the Solver"""
    question: str
    pseudo_label: str
    confidence: float
    iteration: int
    solver_accuracy: float


class SolverAgent:
    """Solver agent that learns to solve math problems"""

    def __init__(self, model_manager, config, tokenizer=None):
        """Initialize the Solver agent

        Args:
            model_manager: UnslothModelManager for model handling
            config: SolverConfig with hyperparameters
            tokenizer: Optional tokenizer (uses model_manager's if not provided)
        """
        self.model_manager = model_manager
        self.config = config
        self.tokenizer = tokenizer or model_manager.get_tokenizer()
        self.current_iteration = 0
        self.training_history = []

        # Compile regex patterns
        self._compile_patterns()

    def _compile_patterns(self):
        """Compile regex patterns for answer extraction"""
        # Pattern to extract Final Answer
        self.final_answer_pattern = re.compile(
            r"Final Answer:\s*([-\d,.]+)",
            flags=re.IGNORECASE
        )

        # Alternative answer patterns
        self.answer_patterns = [
            re.compile(r"answer[:\s]+\$?([-\d,.]+)", re.IGNORECASE),
            re.compile(r"total[:\s]+\$?([-\d,.]+)", re.IGNORECASE),
            re.compile(r"result[:\s]+\$?([-\d,.]+)", re.IGNORECASE),
            re.compile(r"=\s*([-\d,.]+)\s*$", re.IGNORECASE),
        ]

        # Pattern to extract numbers
        self.number_pattern = re.compile(r"[-\d,.]+")

    def generate_dataset_with_voting(
        self, questions: List[str], generate_pseudo_labels: bool = True
    ) -> List[SolverDataPoint]:
        """Generate training dataset by solving questions and creating pseudo-labels

        Args:
            questions: List of questions from the Challenger
            generate_pseudo_labels: Whether to generate pseudo-labels via voting

        Returns:
            List of filtered training examples
        """
        logger.info(f"Generating dataset from {len(questions)} questions")

        dataset = []
        model = self.model_manager.get_model()

        for i, question in enumerate(questions):
            if i % 100 == 0:
                logger.info(f"Processing question {i}/{len(questions)}")

            # Generate multiple responses for voting
            responses = []
            for _ in range(self.config.num_responses_for_voting):
                response = self._generate_single_response(question, model)
                responses.append(response)

            # Extract answers and vote for pseudo-label
            answers = []
            for response in responses:
                answer = self._extract_answer(response)
                if answer:
                    answers.append(answer)

            if not answers:
                # Skip if no valid answers
                continue

            # Calculate pseudo-label via majority vote
            answer_counts = Counter(answers)
            pseudo_label, count = answer_counts.most_common(1)[0]

            # Calculate empirical accuracy
            accuracy = count / len(responses)

            # Apply difficulty filtering: keep only if accuracy is in target range
            if (self.config.min_accuracy <= accuracy <= self.config.max_accuracy):
                datapoint = SolverDataPoint(
                    question=question,
                    pseudo_label=pseudo_label,
                    confidence=accuracy,
                    iteration=self.current_iteration,
                    solver_accuracy=accuracy
                )
                dataset.append(datapoint)

        logger.info(f"Filtered dataset contains {len(dataset)} examples")
        logger.info(f"Average confidence: {np.mean([d.confidence for d in dataset]):.3f}")

        return dataset

    def _generate_single_response(self, question: str, model) -> str:
        """Generate a single response for a question"""
        # Format prompt
        prompt = self._format_prompt(question)

        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=self.config.max_prompt_length)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=self.config.max_completion_length,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Remove prompt from response
        response = response[len(prompt):].strip()

        return response

    def _format_prompt(self, question: str) -> str:
        """Format question into prompt for the model"""
        system_prompt = """You are a mathematical reasoning assistant. Solve problems step-by-step.

Show your work clearly and end with:
Final Answer: [numeric value]"""

        # Check if question is already in chat format
        if "<question>" in question:
            # Extract question from tags
            match = re.search(r"<question>(.*?)</question>", question, re.DOTALL)
            if match:
                question = match.group(1).strip()

        # Format as chat
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]

        # Apply chat template
        formatted = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return formatted

    def _extract_answer(self, response: str) -> Optional[str]:
        """Extract numeric answer from response"""
        # Try Final Answer pattern first
        match = self.final_answer_pattern.search(response)
        if match:
            return match.group(1).replace(",", "")

        # Try alternative patterns
        for pattern in self.answer_patterns:
            match = pattern.search(response)
            if match:
                return match.group(1).replace(",", "")

        # Last resort: find last number
        numbers = self.number_pattern.findall(response)
        if numbers:
            return numbers[-1].replace(",", "")

        return None

    def train_with_grpo(self, dataset: List[SolverDataPoint], num_steps: int = None):
        """Train the Solver using GRPO with verifiable rewards

        Args:
            dataset: Training dataset with pseudo-labels
            num_steps: Number of training steps (uses config default if None)
        """
        from trl import GRPOTrainer as TRLGRPOTrainer, GRPOConfig

        num_steps = num_steps or self.config.max_steps
        logger.info(f"Training Solver with GRPO for {num_steps} steps on {len(dataset)} examples")

        # Prepare GRPO dataset format
        grpo_dataset = []
        for datapoint in dataset:
            grpo_dataset.append({
                "prompt": [
                    {"role": "system", "content": "You are a mathematical reasoning assistant. Solve problems step-by-step.\n\nShow your work clearly and end with:\nFinal Answer: [numeric value]"},
                    {"role": "user", "content": datapoint.question}
                ],
                "answer": datapoint.pseudo_label,
                "confidence": datapoint.confidence
            })

        # Prepare training configuration
        # Ensure batch size is at least 1
        batch_size = max(1, self.config.batch_size)

        grpo_config = GRPOConfig(
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            warmup_ratio=self.config.warmup_ratio,
            max_steps=num_steps,
            per_device_train_batch_size=batch_size,  # Use full batch size, not divided
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            num_generations=self.config.num_generations_per_prompt,
            max_prompt_length=self.config.max_prompt_length,
            max_completion_length=self.config.max_completion_length,
            optim="adamw_8bit",
            logging_steps=1,
            report_to="none",
            output_dir=f"outputs/solver_iter_{self.current_iteration}",
            max_grad_norm=0.3,  # Aggressive clipping for stability
        )

        # Create reward function for GRPO
        def verifiable_reward_func(completions, answer=None, confidence=None, **kwargs):
            """Binary reward based on answer correctness"""
            rewards = []

            # Convert answer to list if needed
            if answer is not None and not isinstance(answer, list):
                answer = [answer] * len(completions)

            # Convert confidence to list if needed
            if confidence is not None and not isinstance(confidence, list):
                confidence = [confidence] * len(completions)

            for i, comp in enumerate(completions):
                # Extract text from completion
                if isinstance(comp, list) and len(comp) > 0:
                    text = comp[0].get('content', '')
                else:
                    text = str(comp)

                # Extract model's answer
                model_answer = self._extract_answer(text)

                if not model_answer:
                    # Heavy penalty for no answer
                    rewards.append(-5.0)
                    continue

                # Get correct answer
                if answer and i < len(answer):
                    correct_answer = str(answer[i]).strip()

                    # Check if answer is correct
                    try:
                        # Try numeric comparison
                        model_num = float(model_answer)
                        correct_num = float(correct_answer)

                        if abs(model_num - correct_num) < 0.01:
                            # Correct answer - scale reward by confidence
                            base_reward = 5.0
                            if confidence and i < len(confidence):
                                # Higher reward for high-confidence correct answers
                                conf = confidence[i]
                                reward = base_reward * (0.5 + conf)
                            else:
                                reward = base_reward
                            rewards.append(reward)
                        else:
                            # Wrong answer
                            rewards.append(-3.0)
                    except:
                        # Fallback to string comparison
                        if model_answer == correct_answer:
                            rewards.append(5.0)
                        else:
                            rewards.append(-3.0)
                else:
                    # No target answer available
                    rewards.append(0.0)

                # Bonus for good formatting
                if "Final Answer:" in text:
                    rewards[-1] += 1.0

                # Bonus for step-by-step reasoning
                if any(indicator in text.lower() for indicator in ['step', 'first', 'then', 'therefore']):
                    rewards[-1] += 0.5

            return rewards

        # Initialize GRPO trainer
        model = self.model_manager.get_model()
        trainer = TRLGRPOTrainer(
            model=model,
            processing_class=self.tokenizer,
            reward_funcs=[verifiable_reward_func],
            args=grpo_config,
            train_dataset=grpo_dataset,
        )

        # Train
        train_output = trainer.train()

        # Update iteration counter
        self.current_iteration += 1

        # Store training metrics
        self.training_history.append({
            "iteration": self.current_iteration,
            "loss": train_output.metrics.get('train_loss', 0),
            "reward": train_output.metrics.get('train_reward', 0),
            "dataset_size": len(dataset),
        })

        logger.info(f"Solver training completed. Loss: {train_output.metrics.get('train_loss', 0):.4f}")

        return train_output.metrics

    def evaluate_on_dataset(self, eval_questions: List[str], eval_answers: List[str]) -> Dict[str, float]:
        """Evaluate Solver performance on a dataset

        Args:
            eval_questions: List of evaluation questions
            eval_answers: List of correct answers

        Returns:
            Dictionary of evaluation metrics
        """
        logger.info(f"Evaluating Solver on {len(eval_questions)} questions")

        model = self.model_manager.get_model()
        correct = 0
        total = 0

        for question, true_answer in zip(eval_questions, eval_answers):
            # Generate response
            response = self._generate_single_response(question, model)

            # Extract answer
            model_answer = self._extract_answer(response)

            if model_answer:
                # Clean true answer (remove #### marker if present)
                if "####" in true_answer:
                    true_answer = true_answer.split("####")[-1].strip()

                # Check correctness
                try:
                    model_num = float(model_answer)
                    true_num = float(true_answer)

                    if abs(model_num - true_num) < 0.01:
                        correct += 1
                except:
                    # String comparison
                    if model_answer == true_answer:
                        correct += 1

            total += 1

            # Log progress
            if total % 50 == 0:
                logger.info(f"Evaluated {total}/{len(eval_questions)}, accuracy: {correct/total:.3f}")

        accuracy = correct / total if total > 0 else 0.0

        metrics = {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "iteration": self.current_iteration,
        }

        logger.info(f"Evaluation complete. Accuracy: {accuracy:.3f} ({correct}/{total})")

        return metrics

    def save_checkpoint(self, path: str):
        """Save Solver checkpoint"""
        import os
        import json

        os.makedirs(path, exist_ok=True)

        # Save model
        model = self.model_manager.get_model()
        self.model_manager.save_model(model, os.path.join(path, "model"))

        # Save metadata
        metadata = {
            "iteration": self.current_iteration,
            "training_history": self.training_history,
            "config": self.config.__dict__,
        }

        with open(os.path.join(path, "metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        logger.info(f"Solver checkpoint saved to {path}")

    def load_checkpoint(self, path: str):
        """Load Solver checkpoint"""
        import os
        import json

        # Load model - recreate with LoRA from saved weights
        # For now, just log that loading is not fully implemented
        # In production, you would reload the base model and apply the saved LoRA weights
        logger.info(f"Note: Model checkpoint loading not fully implemented yet")

        # Load metadata
        with open(os.path.join(path, "metadata.json"), "r") as f:
            metadata = json.load(f)

        self.current_iteration = metadata["iteration"]
        self.training_history = metadata.get("training_history", [])
        logger.info(f"Solver checkpoint loaded from {path}")

    def get_model_for_challenger(self):
        """Get the current Solver model for Challenger's uncertainty calculation"""
        return self.model_manager.get_model()
