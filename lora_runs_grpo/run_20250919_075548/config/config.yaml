dataset:
  cache_dir: cache/datasets
  eval_size: 100
  max_seq_length: 256
  seed: 42
  sources:
  - gsm8k
  train_size: 500
  use_cache: true
evolution:
  crossover_rate: 0.3
  generations: 5
  keep_top: 1
  mutation_rate: 0.4
  population_size: 3
grpo:
  enabled: true
  format_examples: 25
  max_completion_length: 256
  max_prompt_length: 256
  max_steps: 50
  num_generations: 4
  pre_train_format: true
  reasoning_end: </think>
  reasoning_start: <think>
  reward_weights:
    accuracy: 3.0
    format: 1.0
    reasoning: 2.0
  solution_end: </solution>
  solution_start: <solution>
  temperature: 0.7
logging:
  level: INFO
  save_history: true
  track_metrics:
  - accuracy
  - perplexity
  - fitness_score
  - rewards
lora_search_space:
  alpha_multiplier:
  - 1
  - 2
  dropout:
  - 0.0
  learning_rate:
  - 5e-5
  - 1e-4
  max_grad_norm:
  - 1.0
  - 2.0
  rank:
  - 8
  - 16
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  target_modules_preset:
  - minimal
  - standard
  - extended
  use_gradient_checkpointing: true
  use_rslora:
  - false
  - true
  warmup_ratio:
  - 0.0
  - 0.1
  weight_decay:
  - 0.0
  - 0.01
mode: self_supervised
model:
  backend: unsloth
  chat_template: qwen3-instruct
  device_map: cuda:0
  load_in_4bit: true
  low_cpu_mem_usage: true
  max_seq_length: 1024
  path: Qwen/Qwen3-4B-Instruct-2507
  quantization: 4bit
  torch_dtype: float16
  trust_remote_code: true
  type: transformers
  use_cache: false
output:
  base_dir: lora_runs_grpo
  keep_runs: 3
  run_name: null
output_dir: evolved_adapters
quick_test:
  enabled: false
  eval_size: null
  generations: 2
  population_size: 2
  train_size: 100
training:
  batch_size: 4
  bf16: false
  dataloader_num_workers: 0
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2
  epochs_per_variant: 1
  fp16: false
  gradient_accumulation_steps: 4
  gradient_checkpointing: false
  max_grad_norm: 1.0
  method: grpo
  warmup_ratio: 0.1
  weight_decay: 0.01
