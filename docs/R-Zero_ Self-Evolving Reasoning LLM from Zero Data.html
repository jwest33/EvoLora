<!DOCTYPE html>
<!-- saved from url=(0035)https://arxiv.org/html/2508.05004v2 -->
<html lang="en" data-theme="dark"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>R-Zero: Self-Evolving Reasoning LLM from Zero Data</title>
<!--Generated on Wed Aug 27 02:33:47 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport">
<link href="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/bootstrap.min.css" rel="stylesheet" type="text/css">
<link href="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css">
<link href="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css">
<link href="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css">
<script src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/bootstrap.bundle.min.js.download"></script>
<script src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/html2canvas.min.js.download"></script>
<script src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/addons_new.js.download"></script>
<script src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/feedbackOverlay.js.download"></script>
<!--<base href="/html/2508.05004v2/">--><base href="."><link rel="stylesheet" href="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/utz6mli.css"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-16x16.png" sizes="16x16"><link rel="icon" type="image/png" href="https://static.arxiv.org/static/browse/0.3.4/images/icons/favicon-32x32.png" sizes="32x32"></head>
<body><header class="mob_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
        <img alt="logo" class="logomark" role="presentation" width="100" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/arxiv-logomark-small-white.svg">
        <span class="sr-only">Back to arXiv</span>
      </a>
    </div>

    <!--TOC, dark mode, links-->
    <div class="html-header-nav">
      <!--back to abstract-->
      
        <a class="nav-link ar5iv-footer-button hover-effect" aria-label="Back to abstract page" href="https://arxiv.org/abs/2508.05004v2">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 512 512" fill="#ffffff" aria-hidden="true">
            <path d="M502.6 278.6c12.5-12.5 12.5-32.8 0-45.3l-128-128c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3L402.7 224 192 224c-17.7 0-32 14.3-32 32s14.3 32 32 32l210.7 0-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l128-128zM160 96c17.7 0 32-14.3 32-32s-14.3-32-32-32L96 32C43 32 0 75 0 128L0 384c0 53 43 96 96 96l64 0c17.7 0 32-14.3 32-32s-14.3-32-32-32l-64 0c-17.7 0-32-14.3-32-32l0-256c0-17.7 14.3-32 32-32l64 0z"></path>
        </svg>
        </a>
      <!--dark mode-->
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode" aria-label="System preference">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
      <!--nav-->
      <button class="navbar-toggler ar5iv-footer-button" type="button" data-bs-theme="dark" data-bs-toggle="collapse" aria-expanded="false" data-bs-target=".ltx_page_main &gt;.ltx_TOC.mobile" aria-controls="navbarSupportedContent" aria-label="Toggle navigation" style="border:none; margin-right: 0em;">
        <svg xmlns="http://www.w3.org/2000/svg" height="1.25em" viewBox="0 0 448 512" aria-hidden="true" role="img" fill="#ffffff"><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"></path></svg>
      </button>
    </div>
    </header><header class="desktop_header">
    <div class="html-header-logo">
      <a href="https://arxiv.org/">
          <img alt="logo" class="logo" role="presentation" width="100" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/arxiv-logo-one-color-white.svg">
          <span class="sr-only">Back to arXiv</span>
      </a>
    </div>
    <div class="html-header-message" role="banner">
        <p>This is <strong>experimental HTML</strong> to improve accessibility. We invite you to report rendering errors. <span class="sr-only">Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.</span> Learn more <a href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">about this project</a> and <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">help improve conversions</a>.
        </p>
    </div>
    <nav class="html-header-nav">
      <a class="ar5iv-footer-button hover-effect" href="https://info.arxiv.org/about/accessible_HTML.html" target="_blank">Why HTML?</a>
      <a class="ar5iv-footer-button hover-effect" target="_blank" href="https://arxiv.org/html/2508.05004v2/#myForm" onclick="event.preventDefault(); var modal = document.getElementById(&#39;myForm&#39;); modal.style.display = &#39;block&#39;; bugReportState.setInitiateWay(&#39;Header&#39;);">Report Issue</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/abs/2508.05004v2">Back to Abstract</a>
      <a class="ar5iv-footer-button hover-effect" href="https://arxiv.org/pdf/2508.05004v2" target="_blank">Download PDF</a>
      <a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle dark/light mode">
        <label id="automatic-tog" class="toggle-icon" title="Switch to light mode" for="__palette_3">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"></path></svg>
        </label>
        <label id="light-tog" class="toggle-icon" title="Switch to dark mode" for="__palette_1" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
        <label id="dark-tog" class="toggle-icon" title="Switch to system preference" for="__palette_2" hidden="">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
        </label>
      </a>
    </nav></header>

<div class="ltx_page_main" id="main">
<nav class="ltx_TOC active" aria-labelledby="toc_header"><h2 id="toc_header" class="sr-only">Table of Contents</h2>

      <div id="listIcon" type="button" class="hide">
          <svg width="17px" height="17px" viewBox="0 0 512 512" style="pointer-events: none;">
          <path d="M40 48C26.7 48 16 58.7 16 72v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V72c0-13.3-10.7-24-24-24H40zM192 64c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zm0 160c-17.7 0-32 14.3-32 32s14.3 32 32 32H480c17.7 0 32-14.3 32-32s-14.3-32-32-32H192zM16 232v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V232c0-13.3-10.7-24-24-24H40c-13.3 0-24 10.7-24 24zM40 368c-13.3 0-24 10.7-24 24v48c0 13.3 10.7 24 24 24H88c13.3 0 24-10.7 24-24V392c0-13.3-10.7-24-24-24H40z"></path>
          </svg>
      </div>
      <div id="arrowIcon" type="button">
          <svg width="17px" height="17px" viewBox="0 0 448 512" style="pointer-events: none;">
          <path d="M9.4 233.4c-12.5 12.5-12.5 32.8 0 45.3l160 160c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L109.2 288 416 288c17.7 0 32-14.3 32-32s-14.3-32-32-32l-306.7 0L214.6 118.6c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0l-160 160z"></path>
          </svg>
      </div><ol class="ltx_toclist"><li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#abstract" title="Abstract">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        Abstract
      </span>
    </a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S1" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S2" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S2.SS1" title="In 2 Preliminaries ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Group Relative Policy Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S2.SS1.SSS0.Px1" title="In 2.1 Group Relative Policy Optimization ‣ 2 Preliminaries ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Policy Update.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S2.SS2" title="In 2 Preliminaries ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Reinforcement Learning with Verifiable Rewards</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS1" title="In 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2" title="In 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Challenger Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2.SSS0.Px1" title="In 3.2 Challenger Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Uncertainty Reward.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2.SSS0.Px2" title="In 3.2 Challenger Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Repetition Penalty.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2.SSS0.Px3" title="In 3.2 Challenger Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Format Check Penalty.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2.SSS0.Px4" title="In 3.2 Challenger Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Composite Reward and Policy Update.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS3" title="In 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Solver Dataset Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS4" title="In 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Solver Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS5" title="In 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Theoretical Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1" title="In 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiments Setting</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1.SSS1" title="In 4.1 Experiments Setting ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1.SSS2" title="In 4.1 Experiments Setting ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Evaluation Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1.SSS2.Px1" title="In 4.1.2 Evaluation Benchmark ‣ 4.1 Experiments Setting ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">Mathematical Reasoning.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1.SSS2.Px2" title="In 4.1.2 Evaluation Benchmark ‣ 4.1 Experiments Setting ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">General Domain Reasoning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS1.SSS3" title="In 4.1 Experiments Setting ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Training Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS2" title="In 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results in Mathematical Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.SS3" title="In 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results in General Reasoning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS1" title="In 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS2" title="In 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evolution of Question Difficulty and Data Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS3" title="In 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Synergy with Supervised Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS4" title="In 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Iteration Scaling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS4.SSS1" title="In 5.4 Iteration Scaling ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span>The Inevitability of Collapse: An Empirical Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS4.SSS2" title="In 5.4 Iteration Scaling ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span>Beyond Label Noise: Unpacking the Roots of Instability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS5" title="In 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Parameter Sharing
Between Challenger and Solver</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S6" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S6.SS1" title="In 6 Related Work ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Label-Free Reinforcement Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S6.SS2" title="In 6 Related Work ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Self-Play in Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S6.SS3" title="In 6 Related Work ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Reinforcement Learning with Verifiable Rewards (RLVR)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S7" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1" title="In R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experiment Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS1" title="In Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Training Hyperparameter</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS1.SSS1" title="In A.1 Training Hyperparameter ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>Solver Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS1.SSS2" title="In A.1 Training Hyperparameter ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>Challenger Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS2" title="In Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Prompt Templates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS3" title="In Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>GPT-4o Judge Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS4" title="In Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Repetition Penalty Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS4.SSS0.Px1" title="In A.4 Repetition Penalty Implementation ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">1. Pairwise Distance Calculation via BLEU Score</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS4.SSS0.Px2" title="In A.4 Repetition Penalty Implementation ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">2. Agglomerative Clustering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS4.SSS0.Px3" title="In A.4 Repetition Penalty Implementation ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_title">3. Final Penalty Calculation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
    <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib" title="References">
      <span class="ltx_text ltx_ref_title">
        <span class="ltx_tag ltx_tag_ref"></span>
        References
      </span>
    </a></li></ol></nav>

<div class="ltx_page_content"><div class="package-alerts ltx_document" role="status" aria-label="Conversion errors have been found" style="display: none;">
      <button aria-label="Dismiss alert" onclick="closePopup()">
          <span aria-hidden="true"><svg role="presentation" width="20" height="20" viewBox="0 0 44 44" aria-hidden="true" focusable="false">
          <path d="M0.549989 4.44999L4.44999 0.549988L43.45 39.55L39.55 43.45L0.549989 4.44999Z"></path>
          <path d="M39.55 0.549988L43.45 4.44999L4.44999 43.45L0.549988 39.55L39.55 0.549988Z"></path>
          </svg></span>
      </button>
      <p>HTML conversions <a href="https://info.dev.arxiv.org/about/accessibility_html_error_messages.html" target="_blank">sometimes display errors</a> due to content that did not convert correctly from the source. This paper uses the following packages that are not yet supported by the HTML conversion tool. Feedback on these issues are not necessary; they are known and are being worked on.</p>
          <ul arial-label="Unsupported packages used in this paper">
              <li>failed: arydshln.sty</li>
          </ul>
      <p>Authors: achieve the best HTML results from your LaTeX submissions by following these <a href="https://info.arxiv.org/help/submit_latex_best_practices.html" target="_blank">best practices</a>.</p>
    </div><div id="target-section" class="section"><a id="license-tr" href="https://info.arxiv.org/help/license/index.html#licenses-available">License: CC BY-NC-ND 4.0</a><div id="watermark-tr">arXiv:2508.05004v2 [cs.LG] 27 Aug 2025</div></div>
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_inline-block">
<span class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_bold ltx_font_italic">R-Zero</span>: Self-Evolving Reasoning LLM from Zero Data</span>
</span>
</h1><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chengsong Huang<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,2</span></sup><sup class="ltx_sup">†</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenhao Yu<sup class="ltx_sup">1</sup><sup class="ltx_sup">†</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Xiaoyang Wang<sup class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongming Zhang<sup class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Zongxia Li<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">1,3</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><span class="ltx_text ltx_font_bold">Ruosen Li<sup class="ltx_sup"><span class="ltx_text ltx_font_medium ltx_font_italic">1,4</span></sup>, Jiaxin Huang<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">2</span></sup>, Haitao Mi<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">1</span></sup>, Dong Yu<sup class="ltx_sup"><span class="ltx_text ltx_font_medium">1</span></sup></span>
<br class="ltx_break"><sup class="ltx_sup">1</sup>Tencent AI Seattle Lab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<sup class="ltx_sup">2</sup>Washington University in St. Louis
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><sup class="ltx_sup">3</sup>University of Maryland
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> College Park
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<sup class="ltx_sup">4</sup>The University of Texas at Dallas

<br class="ltx_break"><math alttext="\dagger" class="ltx_Math" display="inline" id="m16"><semantics><mo>†</mo><annotation encoding="application/x-tex">\dagger</annotation></semantics></math> Core contributors

<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">chengsong@wustl.edu; wenhaowyu@global.tencent.com</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_abstract" id="abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p">Self-evolving Large Language Models (LLMs) offer a scalable path toward superintelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence.
To overcome this limitation, we introduce <span class="ltx_text ltx_font_italic">R-Zero</span>, a fully autonomous framework that generates its own training data from scratch.
Starting from a single base LLM, <span class="ltx_text ltx_font_italic">R-Zero</span> initializes two independent models with distinct roles – a <span class="ltx_text ltx_font_bold">Challenger</span> and a <span class="ltx_text ltx_font_bold">Solver</span>. These models are optimized separately and <span class="ltx_text ltx_font_bold">co-evolve</span> through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver’s capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels.
Empirically, <span class="ltx_text ltx_font_italic">R-Zero</span> substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math reasoning benchmarks, and +7.54 on general-domain reasoning benchmarks.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<p class="ltx_p ltx_align_center"><span class="ltx_text" style="position:relative; bottom:-0.1pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="18" id="g1" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/github.png" width="18"></span>
Code: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Chengsong-Huang/R-Zero" title="">https://github.com/Chengsong-Huang/R-Zero</a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="224" id="S0.F1.g1" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/abstract.png" width="568">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(Left): <span class="ltx_text ltx_font_bold ltx_font_italic">R-Zero</span> employs a co-evolutionary loop between Challenger and Solver. (Right): <span class="ltx_text ltx_font_bold ltx_font_italic">R-Zero</span> achieves strong benchmark gains without any pre-existing tasks or human labels.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p">Self-evolving Large Language Models (LLMs) represent a promising frontier for advancing language intelligence. By autonomously generating, refining, and learning from their own experiences, these models provide a scalable pathway toward artificial superintelligence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib42" title="">2024</a>; Tan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib40" title="">2024a</a>)</cite>.
A critical requirement for training such self-evolving LLMs is access to large volumes of expertly curated tasks and labels, which serve as supervision signals for fine-tuning or reinforcement learning with verifiable rewards (RLVR)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib33" title="">2024</a>; DeepSeek-AI et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib8" title="">2025</a>)</cite>.
However, relying on human annotators to create these tasks and labels is not only costly, labor-intensive, and difficult to scale, but also presents a fundamental bottleneck to advancing AI systems toward capabilities that could eventually surpass human intelligence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Su et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib38" title="">2025</a>; Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib51" title="">2025a</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p">To reduce dependence on human-curated data, self-generated and label-free methods have been proposed to eliminate the need for explicit supervision. In particular, label-free RL derives reward signals directly from the model’s own outputs, such as sequence-level confidence scores&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib22" title="">2025a</a>; Prabhudesai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib29" title="">2025</a>; Huang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib18" title="">2025</a>)</cite> and output entropy&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Agarwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib1" title="">2025</a>; Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib5" title="">2025</a>)</cite>.
However, despite removing the need for explicit labels, label-free methods still relies on a pre-existing corpus of tasks, which limits its scalability in truly self-evolving settings.
On the other side, self-challenging approaches train LLMs on tasks generated by the models themselves&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib56" title="">2025a</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib43" title="">2025a</a>; Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib51" title="">2025a</a>)</cite>,
While promising, many of these methods rely on external code executors to ensure that the synthesized tasks are both feasible and verifiable. However, in domains that lack an explicit verification oracle, such as open-ended reasoning, ensuring the quality and correctness of self-generated data remains a significant challenge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p">In this paper, we propose <span class="ltx_text ltx_font_italic">R-Zero</span>, a framework for training reasoning LLMs that can self-evolve from zero external data.
In <span class="ltx_text ltx_font_italic">R-Zero</span>, a single base model is initialized with two roles – a <span class="ltx_text ltx_font_bold">Challenger</span> and a <span class="ltx_text ltx_font_bold">Solver</span> that are independently optimized but <span class="ltx_text ltx_font_bold">co-evolve</span> throughout the RL process.
During co-evolving, the Challenger is rewarded for generating tasks targeted to be at the edge of Solver’s current abilities, while the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. Framework details are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3" title="3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3</span></a>, but briefly, in the Challenger training phase, the Challenger is trained via Group Relative Policy Optimization (GRPO)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib33" title="">2024</a>)</cite> to generate difficult questions. The reward signal is derived from the uncertainty for the frozen Solver, which is measured by the self-consistency of its multiple generated answers. In the Solver training phase, the Solver is fine-tuned with GRPO on a filtered set of these challenging questions generated by the now-frozen Challenger, using the pseudo-labels voted by itself. This entire process repeats, creating a self-evolving cycle that operates without any human intervention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p">Our experiments demonstrate that <span class="ltx_text ltx_font_italic">R-Zero</span> is a model-agnostic framework, consistently and iteratively improving the reasoning abilities of different backbone LLMs. For example, Qwen3-4B-Base model’s average score on math benchmarks increased by a significant <span class="ltx_text ltx_font_bold">+6.49</span> points after three iterations of self-evolution. Moreover, the reasoning skills learned through our math-focused questions can generalize to complex general-domain tasks, with models trained using <span class="ltx_text ltx_font_italic">R-Zero</span> showing significant improvements on general domain reasoning benchmarks like MMLU-Pro&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib44" title="">2024</a>)</cite> and SuperGPQA&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Du et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib11" title="">2025</a>)</cite>. Our further analysis finds that <span class="ltx_text ltx_font_italic">R-Zero</span> can act as a mid-training method, as models first improved by our method achieve higher performance after fine-tuned on labeled data.
In addition, we provide an in-depth analysis that validates our framework’s components, demonstrates its synergy with supervised fine-tuning, and characterizes the co-evolutionary dynamics to identify both strengths and limitations, offering insights for future research.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p">Our work builds upon recent advancements in reinforcement learning for fine-tuning large language models. We briefly review two key methodologies that are relevant to our framework.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Group Relative Policy Optimization</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p">Group Relative Policy Optimization (GRPO)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib33" title="">2024</a>)</cite> is a reinforcement learning algorithm that fine-tunes a policy LLM <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S2.SS1.p1.m1"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> without a separate, learned value function. Its core idea is to normalize rewards based on the performance of other responses generated from the same prompt.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p">For a given prompt <math alttext="p" class="ltx_Math" display="inline" id="S2.SS1.p2.m1"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, a policy LLM <math alttext="\pi_{\theta_{\text{old}}}" class="ltx_Math" display="inline" id="S2.SS1.p2.m2"><semantics><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><annotation encoding="application/x-tex">\pi_{\theta_{\text{old}}}</annotation></semantics></math> generates a group of <math alttext="G" class="ltx_Math" display="inline" id="S2.SS1.p2.m3"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> complete responses <math alttext="\{x_{1},\dots,x_{G}\}" class="ltx_Math" display="inline" id="S2.SS1.p2.m4"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>x</mi><mi>G</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{x_{1},\dots,x_{G}\}</annotation></semantics></math>. Each response <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS1.p2.m5"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> is evaluated to receive a single scalar reward <math alttext="r_{i}" class="ltx_Math" display="inline" id="S2.SS1.p2.m6"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math>. The rewards across the group are then normalized using a z-score to compute a response-level advantage:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{A}_{i}=\frac{r_{i}-\operatorname{mean}(r_{1},\dots,r_{G})}{\operatorname{std}(r_{1},\dots,r_{G})+\varepsilon_{\text{norm}}}," class="ltx_Math" display="block" id="S2.Ex1.m1"><semantics><mrow><mrow><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>−</mo><mrow><mi>mean</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mrow><mrow><mi>std</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msub><mi>ε</mi><mtext>norm</mtext></msub></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\hat{A}_{i}=\frac{r_{i}-\operatorname{mean}(r_{1},\dots,r_{G})}{\operatorname{std}(r_{1},\dots,r_{G})+\varepsilon_{\text{norm}}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\varepsilon_{\text{norm}}" class="ltx_Math" display="inline" id="S2.SS1.p2.m7"><semantics><msub><mi>ε</mi><mtext>norm</mtext></msub><annotation encoding="application/x-tex">\varepsilon_{\text{norm}}</annotation></semantics></math> is a small constant added for numerical stability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Policy Update.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p">The policy is updated using a clipped surrogate objective, similar to PPO, to ensure stable training. The objective, regularized by a KL-divergence penalty to constrain policy drift, is:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{GRPO}}(\theta)=-\frac{1}{G}\sum_{i=1}^{G}\min\!\Bigl{(}\tfrac{\pi_{\theta}(x_{i})}{\pi_{\theta_{\text{old}}}(x_{i})}\,\hat{A}_{i},\;\operatorname{clip}\!\bigl{(}\tfrac{\pi_{\theta}(x_{i})}{\pi_{\theta_{\text{old}}}(x_{i})},\,1-\epsilon,\,1+\epsilon\bigr{)}\,\hat{A}_{i}\Bigr{)}\;+\;\beta\,\operatorname{KL}\!\bigl{(}\pi_{\theta}\|\,\pi_{\theta_{\text{old}}})." class="ltx_Math" display="block" id="S2.Ex2.m1"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>GRPO</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo>−</mo><mrow><mfrac><mn>1</mn><mi>G</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></munderover><mrow><mpadded width="1.653em"><mi>min</mi></mpadded><mo>⁡</mo><mrow><mo maxsize="1.600em" minsize="1.600em">(</mo><mrow><mstyle displaystyle="false"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo lspace="0.170em" rspace="0em">​</mo><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mo rspace="0.447em">,</mo><mrow><mrow><mpadded width="1.428em"><mi>clip</mi></mpadded><mo>⁡</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mstyle displaystyle="false"><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>,</mo><mrow><mn> 1</mn><mo>−</mo><mi>ϵ</mi></mrow><mo>,</mo><mrow><mn> 1</mn><mo>+</mo><mi>ϵ</mi></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><mo lspace="0.170em" rspace="0em">​</mo><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub></mrow><mo maxsize="1.600em" minsize="1.600em" rspace="0.280em">)</mo></mrow></mrow></mrow></mrow></mrow><mo rspace="0.502em">+</mo><mrow><mi>β</mi><mo lspace="0.337em" rspace="0em">​</mo><mrow><mpadded width="1.431em"><mi>KL</mi></mpadded><mo>⁡</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo rspace="0.448em">∥</mo><msub><mi>π</mi><msub><mi>θ</mi><mtext>old</mtext></msub></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{GRPO}}(\theta)=-\frac{1}{G}\sum_{i=1}^{G}\min\!\Bigl{(}\tfrac{\pi_{\theta}(x_{i})}{\pi_{\theta_{\text{old}}}(x_{i})}\,\hat{A}_{i},\;\operatorname{clip}\!\bigl{(}\tfrac{\pi_{\theta}(x_{i})}{\pi_{\theta_{\text{old}}}(x_{i})},\,1-\epsilon,\,1+\epsilon\bigr{)}\,\hat{A}_{i}\Bigr{)}\;+\;\beta\,\operatorname{KL}\!\bigl{(}\pi_{\theta}\|\,\pi_{\theta_{\text{old}}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Maximizing the negative of this loss encourages the policy to increase the probability of generating responses with positive relative advantages, while the KL term, controlled by <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.m1"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, limits divergence from the previous policy.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Reinforcement Learning with Verifiable Rewards</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p">Reinforcement Learning with Verifiable Rewards (RLVR)&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lambert et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib20" title="">2024</a>)</cite> is a paradigm for fine-tuning models in domains where response quality can be deterministically verified. This approach relies on a rule-based verifier <math alttext="v:\mathcal{X}\to\{0,1\}" class="ltx_Math" display="inline" id="S2.SS2.p1.m1"><semantics><mrow><mi>v</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo stretchy="false">→</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">v:\mathcal{X}\to\{0,1\}</annotation></semantics></math> that assigns a binary reward to each generation <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.p1.m2"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{i}\;=\;v(x_{i})\;=\;\begin{cases}1,&amp;\text{if }x_{i}\text{ satisfies a task-specific correctness check},\\[4.0pt]
0,&amp;\text{otherwise}.\end{cases}" class="ltx_Math" display="block" id="S2.Ex3.m1"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo lspace="0.558em" rspace="0.558em">=</mo><mrow><mi>v</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo rspace="0.280em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.558em">=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;satisfies a task-specific correctness check</mtext></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">r_{i}\;=\;v(x_{i})\;=\;\begin{cases}1,&amp;\text{if }x_{i}\text{ satisfies a task-specific correctness check},\\[4.0pt]
0,&amp;\text{otherwise}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">This reward structure is especially effective for tasks like math, code generation with clear correctness criteria, and serves as the foundation for the reward mechanism in our Solver training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">We propose <span class="ltx_text ltx_font_bold ltx_font_italic">R-Zero</span>, a fully automated framework featuring a <span class="ltx_text ltx_font_bold">Challenger</span> and a <span class="ltx_text ltx_font_bold">Solver</span>, both initialized from the same base LLM. The framework operates in an iterative loop. We illustrate the main framework in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.F2" title="Figure 2 ‣ 3.1 Overview ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">2</span></a>. First, the Challenger (<math alttext="Q_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p1.m1"><semantics><msub><mi>Q</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">Q_{\theta}</annotation></semantics></math>) is trained with Group Relative Policy Optimization (GRPO) to generate synthetic questions that are challenging for the current Solver (Sec.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS2" title="3.2 Challenger Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3.2</span></a>). A training dataset of question-answer pairs is then constructed from these synthetic questions using a filtering strategy and a majority-vote mechanism (Sec.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS3" title="3.3 Solver Dataset Construction ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Next, the Solver (<math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S3.SS1.p1.m2"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math>) is fine-tuned on this new dataset, also using GRPO (Sec.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS4" title="3.4 Solver Training ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3.4</span></a>). This iterative process allows the Challenger and Solver to co-evolve, leading to a progressively more capable Solver. The entire framework is self-supervised, requiring no human intervention.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S3.F2.g1" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/method.png" width="598">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An overview of our <span class="ltx_text ltx_font_italic">R-Zero</span> framework, which illustrates the co-evolution of the Challenger and the Solver.
<span class="ltx_text ltx_font_bold">Top:</span> In the Challenger training phase, the Challenger is trained via GRPO to generate difficult questions. The reward signal is derived from the uncertainty for the frozen Solver, which is measured by the self-consistency of its multiple generated answers.
<span class="ltx_text ltx_font_bold">Bottom:</span> In the Solver training phase, the Solver is fine-tuned with GRPO on a filtered set of these challenging questions generated by the now-frozen Challenger, using the pseudo-labels voted by itself.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Challenger Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p">The Challenger, <math alttext="Q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.m1"><semantics><msub><mi>Q</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">Q_{\theta}</annotation></semantics></math>, is an autoregressive language model trained to generate challenging questions. We train <math alttext="Q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2"><semantics><msub><mi>Q</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">Q_{\theta}</annotation></semantics></math> using the GRPO algorithm detailed in Sec.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S2" title="2 Preliminaries ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">2</span></a>. The core of this process lies in designing a reward function that accurately captures the desired properties of a “good” question. This final scalar reward, <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.m3"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math>, is then used in the GRPO advantage calculation. We focus on generating questions specifically within the domain of mathematics, as it provides a convenient and self-contained setting for our framework; the objective nature of mathematical answers allows for the straightforward generation of pseudo-labels via majority voting, without the need for external verification environments like code executors.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Uncertainty Reward.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p">To guide the Challenger toward producing challenging yet solvable questions, we first define an uncertainty score. For a generated question <math alttext="x" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m1"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, we query the current Solver <math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m2"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math> for <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m3"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> responses <math alttext="\{y_{1},\dots,y_{m}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m4"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{y_{1},\dots,y_{m}\}</annotation></semantics></math>. The most frequent response is treated as the pseudo-label <math alttext="\tilde{y}(x)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m5"><semantics><mrow><mover accent="true"><mi>y</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\tilde{y}(x)</annotation></semantics></math>, and we compute the Solver’s empirical accuracy as <math alttext="\hat{p}(x;S_{\phi})=\frac{1}{m}\sum_{j=1}^{m}\mathbbm{1}\{y_{j}=\tilde{y}(x)\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.m6"><semantics><mrow><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><msub><mi>S</mi><mi>ϕ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><mi>m</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mrow><mn>𝟙</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">{</mo><mrow><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mrow><mover accent="true"><mi>y</mi><mo>~</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{p}(x;S_{\phi})=\frac{1}{m}\sum_{j=1}^{m}\mathbbm{1}\{y_{j}=\tilde{y}(x)\}</annotation></semantics></math>. The uncertainty reward is then defined as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{\text{uncertainty}}(x;\phi)=1-2\left|\hat{p}(x;S_{\phi})-\tfrac{1}{2}\right|" class="ltx_Math" display="block" id="S3.Ex4.m1"><semantics><mrow><mrow><msub><mi>r</mi><mtext>uncertainty</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo>|</mo><mrow><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><msub><mi>S</mi><mi>ϕ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mstyle displaystyle="false"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle></mrow><mo>|</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{\text{uncertainty}}(x;\phi)=1-2\left|\hat{p}(x;S_{\phi})-\tfrac{1}{2}\right|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">This function incentivizes questions where the Solver is maximally uncertain (accuracy approaches 50%). We provide a theoretical motivation for this reward function in Sec.&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS5" title="3.5 Theoretical Analysis ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Repetition Penalty.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p">To encourage diversity within a training batch <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m1"><semantics><mi class="ltx_font_mathcaligraphic">𝒳</mi><annotation encoding="application/x-tex">\mathcal{X}</annotation></semantics></math>, we introduce a repetition penalty.
We could use any similarity metric, but in our case, we specifically use the BLEU score for faster computation, as this calculation must be performed numerous times during the rollout process.
We compute pairwise distances using BLEU score similarity, <math alttext="d_{ij}=1-\text{BLEU}(x_{i},x_{j})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m2"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mtext>BLEU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">d_{ij}=1-\text{BLEU}(x_{i},x_{j})</annotation></semantics></math>, and group questions where <math alttext="d_{ij}&lt;\tau_{\text{BLEU}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m3"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>&lt;</mo><msub><mi>τ</mi><mtext>BLEU</mtext></msub></mrow><annotation encoding="application/x-tex">d_{ij}&lt;\tau_{\text{BLEU}}</annotation></semantics></math> into clusters <math alttext="\mathcal{C}=\{C_{1},\dots,C_{K}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m4"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>C</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{C}=\{C_{1},\dots,C_{K}\}</annotation></semantics></math>. The penalty for a question <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m5"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> in a cluster <math alttext="C_{k}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m6"><semantics><msub><mi>C</mi><mi>k</mi></msub><annotation encoding="application/x-tex">C_{k}</annotation></semantics></math> is proportional to its relative size:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{\text{rep}}(x_{i})=\lambda\frac{|C_{k}|}{B}" class="ltx_Math" display="block" id="S3.Ex5.m1"><semantics><mrow><mrow><msub><mi>r</mi><mtext>rep</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>λ</mi><mo lspace="0em" rspace="0em">​</mo><mfrac><mrow><mo stretchy="false">|</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">|</mo></mrow><mi>B</mi></mfrac></mrow></mrow><annotation encoding="application/x-tex">r_{\text{rep}}(x_{i})=\lambda\frac{|C_{k}|}{B}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="B" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m7"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is the batch size and <math alttext="\lambda" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m8"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> is a scaling factor. In our experiments, we set <math alttext="\lambda=1" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.m9"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda=1</annotation></semantics></math>. The implementation details are shown in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1.SS4" title="A.4 Repetition Penalty Implementation ‣ Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Format Check Penalty.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p">A critical first step in the reward pipeline is a structural format check to verify that each generated question is correctly enclosed within <span class="ltx_text ltx_font_typewriter">&lt;question&gt;</span> and <span class="ltx_text ltx_font_typewriter">&lt;/question&gt;</span> tags. If the output does not adhere to this required structure, it is immediately assigned a final reward of <math alttext="0" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.m1"><mn>0</mn></math>, and no further reward signals are computed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Composite Reward and Policy Update.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p">For all questions that pass the format check, we calculate a composite reward. The final scalar reward <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m1"><semantics><msub><mi>r</mi><mi>i</mi></msub><annotation encoding="application/x-tex">r_{i}</annotation></semantics></math> for each valid question <math alttext="x_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m2"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> combines signals for uncertainty and repetition:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{i}=\max\bigl{(}0,r_{\text{uncertainty}}(x_{i};\phi)-r_{\text{rep}}(x_{i})\bigr{)}" class="ltx_Math" display="block" id="S3.Ex6.m1"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>max</mi><mo>⁡</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mn>0</mn><mo>,</mo><mrow><mrow><msub><mi>r</mi><mtext>uncertainty</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mrow><msub><mi>r</mi><mtext>rep</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{i}=\max\bigl{(}0,r_{\text{uncertainty}}(x_{i};\phi)-r_{\text{rep}}(x_{i})\bigr{)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">With these rewards <math alttext="\{r_{1},\dots,r_{G}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m3"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>r</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>r</mi><mi>G</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{r_{1},\dots,r_{G}\}</annotation></semantics></math> for a batch of generated questions, we compute the advantage <math alttext="\hat{A}_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m4"><semantics><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{A}_{i}</annotation></semantics></math> for each question and update the Challenger’s policy <math alttext="Q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m5"><semantics><msub><mi>Q</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">Q_{\theta}</annotation></semantics></math> by minimizing the GRPO loss <math alttext="\mathcal{L}_{\text{GRPO}}(\theta)" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px4.p1.m6"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>GRPO</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{GRPO}}(\theta)</annotation></semantics></math>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Solver Dataset Construction</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p">After updating the Challenger, we use it to generate a new, curated dataset to train the Solver. This process acts as a curriculum generator. We first sample a large pool of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.p1.m1"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> candidate questions from the Challenger’s policy, <math alttext="x_{i}\sim Q_{\theta}(\cdot\mid p_{0})" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.m2"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∼</mo><msub><mi>Q</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo lspace="0em" rspace="0.167em">∣</mo><msub><mi>p</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{i}\sim Q_{\theta}(\cdot\mid p_{0})</annotation></semantics></math>. For each question, we obtain <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.p1.m3"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> answers from the current Solver, determine the pseudo-label <math alttext="\tilde{y}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.m4"><semantics><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\tilde{y}_{i}</annotation></semantics></math> via majority vote, and calculate the empirical correctness <math alttext="\hat{p}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.m5"><semantics><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{p}_{i}</annotation></semantics></math>. A question-answer pair <math alttext="(x_{i},\tilde{y}_{i})" class="ltx_Math" display="inline" id="S3.SS3.p1.m6"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_{i},\tilde{y}_{i})</annotation></semantics></math> is added to the training set <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS3.p1.m7"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math> only if its correctness falls within an informative band, <math alttext="|\hat{p}_{i}-\tfrac{1}{2}|\leq\delta" class="ltx_Math" display="inline" id="S3.SS3.p1.m8"><semantics><mrow><mrow><mo stretchy="false">|</mo><mrow><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>i</mi></msub><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo stretchy="false">|</mo></mrow><mo>≤</mo><mi>δ</mi></mrow><annotation encoding="application/x-tex">|\hat{p}_{i}-\tfrac{1}{2}|\leq\delta</annotation></semantics></math>. This filtering step discards tasks that are either too easy or too hard.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p">While the primary goal of this filtering is to discard tasks that are too easy or too hard, it also serves as an implicit quality control mechanism. Since our pseudo-labels are derived from a majority vote, a very low empirical correctness <math alttext="\hat{p}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.m1"><semantics><msub><mover accent="true"><mi>p</mi><mo>^</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\hat{p}_{i}</annotation></semantics></math> often indicates that the question itself is ambiguous, ill-posed, or that the resulting pseudo-label is unreliable. By filtering out these low-consistency items, our method simultaneously improves the quality and the uncertainty calibration of the training data.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Solver Training</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p">The Solver, <math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S3.SS4.p1.m1"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math>, is then fine-tuned on the curated dataset of challenging problems <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS4.p1.m2"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math>. We also use GRPO for this stage, but with a simpler, verifiable reward signal. For a given question <math alttext="x_{i}\in\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS4.p1.m3"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi></mrow><annotation encoding="application/x-tex">x_{i}\in\mathcal{S}</annotation></semantics></math> with its pseudo-label <math alttext="\tilde{y}_{i}" class="ltx_Math" display="inline" id="S3.SS4.p1.m4"><semantics><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\tilde{y}_{i}</annotation></semantics></math>, the Solver generates a batch of answers, each assigned a binary reward <math alttext="r_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.m5"><semantics><msub><mi>r</mi><mi>j</mi></msub><annotation encoding="application/x-tex">r_{j}</annotation></semantics></math>:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{j}=\begin{cases}1,&amp;\text{if }x_{j}\text{ is identical to the pseudo-label }\tilde{y}_{i},\\
0,&amp;\text{otherwise}.\end{cases}" class="ltx_Math" display="block" id="S3.Ex7.m1"><semantics><mrow><msub><mi>r</mi><mi>j</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>1</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mtext>if&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><msub><mi>x</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><mtext>&nbsp;is identical to the pseudo-label&nbsp;</mtext><mo lspace="0em" rspace="0em">​</mo><msub><mover accent="true"><mi>y</mi><mo>~</mo></mover><mi>i</mi></msub></mrow><mo>,</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mn>0</mn><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mtext>otherwise</mtext><mo lspace="0em">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">r_{j}=\begin{cases}1,&amp;\text{if }x_{j}\text{ is identical to the pseudo-label }\tilde{y}_{i},\\
0,&amp;\text{otherwise}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">This verifiable reward is used to compute the advantage <math alttext="\hat{A}_{j}" class="ltx_Math" display="inline" id="S3.SS4.p1.m6"><semantics><msub><mover accent="true"><mi>A</mi><mo>^</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\hat{A}_{j}</annotation></semantics></math>, and the Solver’s policy <math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S3.SS4.p1.m7"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math> is subsequently updated by minimizing the GRPO loss <math alttext="\mathcal{L}_{\text{GRPO}}(\phi)" class="ltx_Math" display="inline" id="S3.SS4.p1.m8"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>GRPO</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{GRPO}}(\phi)</annotation></semantics></math>. This process enhances the Solver’s ability to correctly answer the difficult questions generated by its co-evolving Challenger.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Theoretical Analysis</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p">In this section, we provide a theoretical motivation for our uncertainty reward function, <math alttext="r_{\text{uncertainty}}\propto 1-2|\hat{p}(x;S_{\phi})-\tfrac{1}{2}|" class="ltx_Math" display="inline" id="S3.SS5.p1.m1"><semantics><mrow><msub><mi>r</mi><mtext>uncertainty</mtext></msub><mo>∝</mo><mrow><mn>1</mn><mo>−</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">|</mo><mrow><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><msub><mi>S</mi><mi>ϕ</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><mo stretchy="false">|</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">r_{\text{uncertainty}}\propto 1-2|\hat{p}(x;S_{\phi})-\tfrac{1}{2}|</annotation></semantics></math>, which is maximized when the Solver’s success probability, <math alttext="\hat{p}" class="ltx_Math" display="inline" id="S3.SS5.p1.m2"><semantics><mover accent="true"><mi>p</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{p}</annotation></semantics></math>, is 50%. Our analysis is grounded in recent work that formally establishes that the most efficient training occurs when a learner is exposed to tasks at the frontier of its capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib34" title="">2025a</a>; Bae et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib2" title="">2025</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS5.p2">
<p class="ltx_p">The core insight from these studies is that the learning potential of the current Solver, with policy <math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S3.SS5.p2.m1"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math>, can be quantified by the KL divergence to an optimal policy <math alttext="S^{*}" class="ltx_Math" display="inline" id="S3.SS5.p2.m2"><semantics><msup><mi>S</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">S^{*}</annotation></semantics></math>. This divergence, <math alttext="\mathbb{D}_{KL}(S_{\phi}||S^{*})" class="ltx_math_unparsed" display="inline" id="S3.SS5.p2.m3"><semantics><mrow><msub><mi>𝔻</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>ϕ</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>S</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{D}_{KL}(S_{\phi}||S^{*})</annotation></semantics></math>, is lower-bounded by the variance of the Solver’s reward. For the binary reward signal used in our framework, the success probability is <math alttext="\hat{p}" class="ltx_Math" display="inline" id="S3.SS5.p2.m4"><semantics><mover accent="true"><mi>p</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{p}</annotation></semantics></math>. This leads to the specific lower bound:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbb{D}_{KL}(S_{\phi}||S^{*})\geq\frac{\hat{p}(1-\hat{p})}{2\beta^{2}}" class="ltx_math_unparsed" display="block" id="S3.Ex8.m1"><semantics><mrow><msub><mi>𝔻</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><mrow><mo stretchy="false">(</mo><msub><mi>S</mi><mi>ϕ</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>S</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><mo>≥</mo><mfrac><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>β</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbb{D}_{KL}(S_{\phi}||S^{*})\geq\frac{\hat{p}(1-\hat{p})}{2\beta^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\beta" class="ltx_Math" display="inline" id="S3.SS5.p2.m5"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> is the temperature parameter controlling entropy regularization. The right-hand side of the inequality, which is proportional to the reward variance, is maximized precisely when <math alttext="\hat{p}=0.5" class="ltx_Math" display="inline" id="S3.SS5.p2.m6"><semantics><mrow><mover accent="true"><mi>p</mi><mo>^</mo></mover><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\hat{p}=0.5</annotation></semantics></math>.
Therefore, by designing the Challenger’s reward to incentivize questions that push the current Solver towards this point of maximum uncertainty, our framework is theoretically motivated to generate a maximally efficient curriculum in each iteration of the co-evolutionary process.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiments Setting</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Models</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS1.p1">
<p class="ltx_p">We employ the Qwen3-4B-Base&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Yang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib46" title="">2025</a>)</cite> and Qwen3-8B-Base models to assess the impact of scale within a single architectural family. Second, to ensure our approach is effective on a distinct lineage, we utilize the OctoThinker-3B and OctoThinker-8B models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib45" title="">2025b</a>)</cite>.This choice is particularly relevant as <cite class="ltx_cite ltx_citemacro_cite">Wang et&nbsp;al. (<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib45" title="">2025b</a>)</cite> reported that applying RL training directly to Llama models yielded suboptimal results. As the OctoThinker series is continually trained from the Llama-3.1 models&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Dubey et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib12" title="">2024</a>)</cite>, this comprehensive selection allows us to test our framework across different foundational architectures – Qwen vs. Llama.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Evaluation Benchmark</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.p1">
<p class="ltx_p">We assess our framework on a comprehensive suite of benchmarks. Although the question-generator prompt for our method is primarily focused on mathematical problem-solving, a key objective of our evaluation is to explore whether the resulting improvements in reasoning ability can generalize to other domains. Therefore, our evaluation is divided into two main categories.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Mathematical Reasoning.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px1.p1">
<p class="ltx_p">We use seven challenging benchmarks: AMC, Minerva&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib21" title="">2022</a>)</cite>, MATH-500&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib16" title="">2021b</a>)</cite>, GSM8K&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Cobbe et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib6" title="">2021</a>)</cite>, Olympiad-Bench&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(He et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib14" title="">2024</a>)</cite>, AIME-2024, and AIME-2025. For these tasks, where answers can be complex, we employ GPT-4o as a programmatic judge to semantically verify the correctness of the final answer against the ground truth&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib53" title="">2025c</a>)</cite>. For the difficult AMC and AIME benchmarks, we report the <span class="ltx_text ltx_font_bold">mean@32</span> metric. For all other math benchmarks, we report accuracy based on greedy decoding.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">General Domain Reasoning.</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS2.Px2.p1">
<p class="ltx_p">To test for the generalization of reasoning ability, we evaluate on the following challenging benchmarks:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MMLU-Pro</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib44" title="">2024</a>)</cite>: An enhanced version of the MMLU&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib15" title="">2021a</a>)</cite> benchmark, featuring a more challenging suite of multi-task questions designed to provide a stricter evaluation of language model capabilities.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">SuperGPQA</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Du et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib11" title="">2025</a>)</cite>: A large-scale benchmark focused on graduate-level reasoning. It comprises questions across 285 distinct disciplines that have been verified as unsearchable on the web, thereby isolating true reasoning ability from simple knowledge recall.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S4.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">BBEH</span>&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(shoaa kazemi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib36" title="">2025</a>)</cite>: This benchmark builds upon the foundation of BIG-Bench Hard&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Suzgun et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib39" title="">2023</a>)</cite> by incorporating a new selection of tasks specifically engineered to be more difficult, thus providing a more accurate measure of complex reasoning skills.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
<p class="ltx_p">For this category, we follow the experimental setup, prompts, and evaluation codes from <cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib27" title="">2025</a>)</cite>, reporting Exact Match (EM) accuracy obtained via greedy decoding.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Training Details</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS3.p1">
<p class="ltx_p">Our entire framework is implemented based on the EasyR1 codebase&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib55" title="">2025b</a>)</cite>. In each iteration of the <span class="ltx_text ltx_font_italic">R-Zero</span> co-evolutionary loop, we follow a specific set of hyperparameters. The Challenger (<math alttext="Q_{\theta}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m1"><semantics><msub><mi>Q</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">Q_{\theta}</annotation></semantics></math>) first generates a candidate pool of <math alttext="N=8,000" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m2"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>8</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding="application/x-tex">N=8,000</annotation></semantics></math> questions. To construct the training dataset for the Solver, these questions are filtered based on consistency. For each candidate question, we sample <math alttext="m=10" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m3"><semantics><mrow><mi>m</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">m=10</annotation></semantics></math> answers from the current Solver (<math alttext="S_{\phi}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m4"><semantics><msub><mi>S</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">S_{\phi}</annotation></semantics></math>).
A question is retained for the training set only if the number of answers matching the majority-vote pseudo-label is between 3 and 7, inclusive (<math alttext="\delta=0.25" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m5"><semantics><mrow><mi>δ</mi><mo>=</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">\delta=0.25</annotation></semantics></math>). This numerical range is consistent with the methodology used in previous research&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang &amp; Zuo, <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib48" title="">2025</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib24" title="">2025b</a>; Bercovich et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib3" title="">2025</a>)</cite>.
When training the Challenger, the uncertainty reward <math alttext="r(x;\phi)" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m6"><semantics><mrow><mi>r</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>;</mo><mi>ϕ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">r(x;\phi)</annotation></semantics></math> is calculated by sampling <math alttext="m=10" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m7"><semantics><mrow><mi>m</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">m=10</annotation></semantics></math> responses from the Solver. For the intra-batch repetition penalty, we set the clustering distance threshold to <math alttext="\tau_{\text{BLEU}}=0.5" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.m8"><semantics><mrow><msub><mi>τ</mi><mtext>BLEU</mtext></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\tau_{\text{BLEU}}=0.5</annotation></semantics></math>.
Further implementation details and prompts can be found in Appendix&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#A1" title="Appendix A Experiment Details ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">A</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results in Mathematical Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p">The comprehensive results of our experiments are presented in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.T1" title="Table 1 ‣ 4.2 Results in Mathematical Reasoning ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">1</span></a>. The findings confirm that our proposed framework, <span class="ltx_text ltx_font_italic">R-Zero</span>, is a highly effective, model-agnostic method for enhancing the performance of language models on mathematical tasks across different architectures and scales.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comprehensive results on mathematical reasoning benchmarks. We compare each base model against a <span class="ltx_text ltx_font_bold">Base Challenger</span> baseline (where the Solver is trained on questions from an untrained Challenger) and our iterative method, <span class="ltx_text ltx_font_italic">R-Zero</span>. The peak performance achieved during each model’s training process is highlighted in <span class="ltx_text ltx_font_bold">bold</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">AVG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">AMC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Minerva</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">MATH</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">GSM8K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Olympiad</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">AIME25</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">AIME24</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="9"><span class="ltx_text ltx_font_italic">Qwen3-4B-Base</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">42.58</span></td>
<td class="ltx_td ltx_align_center">45.70</td>
<td class="ltx_td ltx_align_center">38.24</td>
<td class="ltx_td ltx_align_center">68.20</td>
<td class="ltx_td ltx_align_center">87.79</td>
<td class="ltx_td ltx_align_center">41.04</td>
<td class="ltx_td ltx_align_center">6.15</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">10.94</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">44.36</span></td>
<td class="ltx_td ltx_align_center">45.00</td>
<td class="ltx_td ltx_align_center">45.22</td>
<td class="ltx_td ltx_align_center">72.80</td>
<td class="ltx_td ltx_align_center">87.87</td>
<td class="ltx_td ltx_align_center">41.19</td>
<td class="ltx_td ltx_align_center">7.29</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">11.15</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">48.06</span></td>
<td class="ltx_td ltx_align_center">51.56</td>
<td class="ltx_td ltx_align_center">51.47</td>
<td class="ltx_td ltx_align_center">78.60</td>
<td class="ltx_td ltx_align_center">91.28</td>
<td class="ltx_td ltx_align_center">43.85</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">9.17</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">10.52</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">48.44</span></td>
<td class="ltx_td ltx_align_center">52.50</td>
<td class="ltx_td ltx_align_center">51.47</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">79.80</span></td>
<td class="ltx_td ltx_align_center">91.66</td>
<td class="ltx_td ltx_align_center">44.30</td>
<td class="ltx_td ltx_align_center">4.27</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">15.10</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">49.07</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">57.27</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">52.94</span></td>
<td class="ltx_td ltx_align_center">79.60</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">92.12</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">44.59</span></td>
<td class="ltx_td ltx_align_center">4.27</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">12.71</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="9"><span class="ltx_text ltx_font_italic">Qwen3-8B-Base</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">49.18</span></td>
<td class="ltx_td ltx_align_center">51.95</td>
<td class="ltx_td ltx_align_center">50.00</td>
<td class="ltx_td ltx_align_center">78.00</td>
<td class="ltx_td ltx_align_center">89.08</td>
<td class="ltx_td ltx_align_center">44.74</td>
<td class="ltx_td ltx_align_center">16.67</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">13.85</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">51.87</span></td>
<td class="ltx_td ltx_align_center">60.70</td>
<td class="ltx_td ltx_align_center">57.72</td>
<td class="ltx_td ltx_align_center">81.60</td>
<td class="ltx_td ltx_align_center">92.56</td>
<td class="ltx_td ltx_align_center">46.44</td>
<td class="ltx_td ltx_align_center">13.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">10.62</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">53.39</span></td>
<td class="ltx_td ltx_align_center">61.56</td>
<td class="ltx_td ltx_align_center">59.93</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">82.00</span></td>
<td class="ltx_td ltx_align_center">93.71</td>
<td class="ltx_td ltx_align_center">48.00</td>
<td class="ltx_td ltx_align_center">14.17</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">14.37</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">53.84</span></td>
<td class="ltx_td ltx_align_center">61.56</td>
<td class="ltx_td ltx_align_center">59.93</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">82.00</span></td>
<td class="ltx_td ltx_align_center">93.93</td>
<td class="ltx_td ltx_align_center">48.30</td>
<td class="ltx_td ltx_align_center">17.60</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">13.54</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">54.69</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">61.67</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">60.66</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">82.00</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">94.09</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">48.89</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">19.17</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">16.35</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="9"><span class="ltx_text ltx_font_italic">OctoThinker-3B</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">26.64</span></td>
<td class="ltx_td ltx_align_center">17.19</td>
<td class="ltx_td ltx_align_center">24.26</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">55.00</span></td>
<td class="ltx_td ltx_align_center">73.69</td>
<td class="ltx_td ltx_align_center">16.15</td>
<td class="ltx_td ltx_align_center">0.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">0.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">27.51</span></td>
<td class="ltx_td ltx_align_center">20.19</td>
<td class="ltx_td ltx_align_center">24.63</td>
<td class="ltx_td ltx_align_center">54.60</td>
<td class="ltx_td ltx_align_center">74.98</td>
<td class="ltx_td ltx_align_center">15.70</td>
<td class="ltx_td ltx_align_center">0.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">2.40</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">27.76</span></td>
<td class="ltx_td ltx_align_center">20.39</td>
<td class="ltx_td ltx_align_center">25.74</td>
<td class="ltx_td ltx_align_center">54.60</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">75.51</span></td>
<td class="ltx_td ltx_align_center">16.30</td>
<td class="ltx_td ltx_align_center">0.10</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">1.67</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">28.20</span></td>
<td class="ltx_td ltx_align_center">24.06</td>
<td class="ltx_td ltx_align_center">25.37</td>
<td class="ltx_td ltx_align_center">54.80</td>
<td class="ltx_td ltx_align_center">74.45</td>
<td class="ltx_td ltx_align_center">17.48</td>
<td class="ltx_td ltx_align_center">0.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">1.25</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">29.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">27.03</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">27.57</span></td>
<td class="ltx_td ltx_align_center">54.20</td>
<td class="ltx_td ltx_align_center">74.98</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">18.22</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">3.23</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">0.00</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="9"><span class="ltx_text ltx_font_italic">OctoThinker-8B</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">36.41</span></td>
<td class="ltx_td ltx_align_center">32.11</td>
<td class="ltx_td ltx_align_center">41.91</td>
<td class="ltx_td ltx_align_center">65.20</td>
<td class="ltx_td ltx_align_center">86.96</td>
<td class="ltx_td ltx_align_center">26.52</td>
<td class="ltx_td ltx_align_center">1.56</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">0.62</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">36.98</span></td>
<td class="ltx_td ltx_align_center">29.30</td>
<td class="ltx_td ltx_align_center">42.28</td>
<td class="ltx_td ltx_align_center">66.20</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">88.10</span></td>
<td class="ltx_td ltx_align_center">27.56</td>
<td class="ltx_td ltx_align_center">1.04</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4.38</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">37.80</span></td>
<td class="ltx_td ltx_align_center">32.97</td>
<td class="ltx_td ltx_align_center">45.22</td>
<td class="ltx_td ltx_align_center">65.60</td>
<td class="ltx_td ltx_align_center">86.96</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">28.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">1.98</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">3.44</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">38.23</span></td>
<td class="ltx_td ltx_align_center">32.58</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">48.53</span></td>
<td class="ltx_td ltx_align_center">67.20</td>
<td class="ltx_td ltx_align_center">87.11</td>
<td class="ltx_td ltx_align_center">27.26</td>
<td class="ltx_td ltx_align_center">0.00</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">4.90</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">38.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">34.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">48.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">68.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">87.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb">27.56</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.42</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">3.44</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p">Our iterative training process consistently and substantially improves upon the performance of the base models. This holds true for large models like Qwen3-8B-Base, where three iterations of <span class="ltx_text ltx_font_italic">R-Zero</span> raise the average performance from a baseline of 49.18 to 54.69, a significant gain of <span class="ltx_text ltx_font_bold">+5.51</span> points. Similarly, on the smaller OctoThinker-3B, our method improves the average score from 26.64 to 29.32 (<span class="ltx_text ltx_font_bold">+2.68</span> points), demonstrating the broad applicability of our self-supervised training loop.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p">This improvement is progressive, with the results showing a clear trend of performance gains across iterations. For instance, the Qwen3-8B-Base model’s average score climbs from a base performance of 49.18 to 53.39 (Iter 1) and ultimately reaches 54.69 (Iter 3). A similar monotonic improvement is observed on OctoThinker-3B, which progresses from its base score of 26.64 to 29.32 after three iterations. This consistent growth underscores the benefits of the co-evolutionary dynamic, where the progressively more capable Solver learns from an increasingly challenging curriculum.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p">The critical role of the Challenger’s RL-based training is validated by the immediate performance leap from the Base Challenger to the first iteration of <span class="ltx_text ltx_font_italic">R-Zero</span>. On Qwen3-8B-Base, this first iteration provides a +1.52 point gain over the baseline, and the improvement is even more pronounced on Qwen3-4B-Base at +3.7 points. This confirms that the intelligent curriculum generated by the RL-trained Challenger is significantly more effective than that of a non-trained generator.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results in General Reasoning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on general-domain reasoning benchmarks. The table compares the Base Model, a <span class="ltx_text ltx_font_bold">Base Challenger</span> baseline, and our iterative <span class="ltx_text ltx_font_italic">R-Zero</span>. The peak performance achieved during each model’s training process is highlighted in <span class="ltx_text ltx_font_bold">bold</span>.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Model Name</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">Overall AVG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">MATH AVG</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t"><span class="ltx_text ltx_font_bold">SuperGPQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt ltx_border_t"><span class="ltx_text ltx_font_bold">MMLU-Pro</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt ltx_border_t"><span class="ltx_text ltx_font_bold">BBEH</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6"><span class="ltx_text ltx_font_italic">Qwen3-4B-Base</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">27.10</span></td>
<td class="ltx_td ltx_align_center">42.58</td>
<td class="ltx_td ltx_align_center">20.88</td>
<td class="ltx_td ltx_align_center">37.38</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">7.57</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">30.83</span></td>
<td class="ltx_td ltx_align_center">44.36</td>
<td class="ltx_td ltx_align_center">24.77</td>
<td class="ltx_td ltx_align_center">47.59</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">6.59</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">34.27</span></td>
<td class="ltx_td ltx_align_center">48.06</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">27.92</span></td>
<td class="ltx_td ltx_align_center">51.69</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">9.42</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">34.92</span></td>
<td class="ltx_td ltx_align_center">48.44</td>
<td class="ltx_td ltx_align_center">27.72</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">53.75</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">9.76</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">34.64</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">49.07</span></td>
<td class="ltx_td ltx_align_center">27.55</td>
<td class="ltx_td ltx_align_center">51.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">10.42</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6"><span class="ltx_text ltx_font_italic">Qwen3-8B-Base</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">34.49</span></td>
<td class="ltx_td ltx_align_center">49.18</td>
<td class="ltx_td ltx_align_center">28.33</td>
<td class="ltx_td ltx_align_center">51.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">8.63</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">36.43</span></td>
<td class="ltx_td ltx_align_center">51.87</td>
<td class="ltx_td ltx_align_center">30.12</td>
<td class="ltx_td ltx_align_center">54.14</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">9.60</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">37.93</span></td>
<td class="ltx_td ltx_align_center">53.39</td>
<td class="ltx_td ltx_align_center">31.26</td>
<td class="ltx_td ltx_align_center">57.17</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">9.91</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">38.45</span></td>
<td class="ltx_td ltx_align_center">53.84</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">31.58</span></td>
<td class="ltx_td ltx_align_center">58.20</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">10.20</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">38.73</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">54.69</span></td>
<td class="ltx_td ltx_align_center">31.38</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">58.23</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">10.60</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="6"><span class="ltx_text ltx_font_italic">OctoThinker-3B</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">12.27</span></td>
<td class="ltx_td ltx_align_center">26.64</td>
<td class="ltx_td ltx_align_center">10.09</td>
<td class="ltx_td ltx_align_center">10.87</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">1.46</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">14.41</span></td>
<td class="ltx_td ltx_align_center">27.51</td>
<td class="ltx_td ltx_align_center">11.19</td>
<td class="ltx_td ltx_align_center">14.53</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">4.40</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">14.93</span></td>
<td class="ltx_td ltx_align_center">27.76</td>
<td class="ltx_td ltx_align_center">12.21</td>
<td class="ltx_td ltx_align_center">15.72</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4.05</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">15.11</span></td>
<td class="ltx_td ltx_align_center">28.20</td>
<td class="ltx_td ltx_align_center">12.43</td>
<td class="ltx_td ltx_align_center">16.08</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">3.74</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">15.67</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">29.32</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">12.44</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">16.71</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">4.20</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="6"><span class="ltx_text ltx_font_italic">OctoThinker-8B</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Model</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">16.81</span></td>
<td class="ltx_td ltx_align_center">32.11</td>
<td class="ltx_td ltx_align_center">13.26</td>
<td class="ltx_td ltx_align_center">20.21</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">1.64</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   Base Challenger</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">25.08</span></td>
<td class="ltx_td ltx_align_center">36.41</td>
<td class="ltx_td ltx_align_center">16.99</td>
<td class="ltx_td ltx_align_center">41.46</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">5.46</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 1)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">26.44</span></td>
<td class="ltx_td ltx_align_center">37.80</td>
<td class="ltx_td ltx_align_center">19.15</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">42.05</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center">6.77</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 2)</th>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">26.77</span></td>
<td class="ltx_td ltx_align_center">38.23</td>
<td class="ltx_td ltx_align_center">19.27</td>
<td class="ltx_td ltx_align_center">41.34</td>
<td class="ltx_td ltx_nopad_r ltx_align_center"><span class="ltx_text ltx_font_bold">8.25</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   <span class="ltx_text ltx_font_italic">R-Zero</span> (Iter 3)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">26.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">38.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">19.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">40.92</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">8.25</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p">Previous work has demonstrated that training language models on reasoning-intensive domains, such as mathematics, can lead to improvements in general-domain capabilities&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Huan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib17" title="">2025</a>)</cite>. A key question, however, is whether this generalization effect still holds when the training curriculum is not human-labeled, but entirely self-generated through <span class="ltx_text ltx_font_italic">R-Zero</span>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p">As shown in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S4.T2" title="Table 2 ‣ 4.3 Results in General Reasoning ‣ 4 Experiments ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">2</span></a>, this transfer of skills is evident across all tested models. For instance, three iterations of our math-focused training improve the average general-domain score of Qwen3-8B-Base by +3.81 points and OctoThinker-3B by +3.65 points. This generalization also extends to the key performance patterns observed in the mathematical results, with progressive iterative gains. This confirms that our method does not merely teach domain-specific knowledge, but enhances the model’s underlying capabilities in a way that successfully generalizes across domains.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p">In this section, we conduct a series of in-depth analyses to better understand the behavior and effectiveness of our <span class="ltx_text ltx_font_italic">R-Zero</span> framework. To ensure consistency, all analytical experiments presented here were conducted on the Qwen3-4B-Base model, unless explicitly stated otherwise.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Ablation Study</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p">To isolate the contribution of each key component within our <span class="ltx_text ltx_font_italic">R-Zero</span> framework, we conduct a comprehensive ablation study on the <span class="ltx_text ltx_font_typewriter">Qwen3-4B-Base</span> model. We specifically investigate the importance of three critical modules by disabling them one at a time and observing the impact on performance. The results are summarized in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.T3" title="Table 3 ‣ 5.1 Ablation Study ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_table ltx_align_floatright" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Ablation study results on the Qwen3-4B-Base model. <span class="ltx_text ltx_font_bold">w/o RL-Challenger</span>: Disables GRPO training for the Challenger. <span class="ltx_text ltx_font_bold">w/o Filtering</span>: Disables the difficulty-based curriculum filtering. <span class="ltx_text ltx_font_bold">w/o Rep. Penalty</span>: Removes the repetition penalty from the Challenger’s reward.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Math AVG</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">General AVG</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">
<span class="ltx_text ltx_font_italic">R-Zero</span> (full)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">48.06</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">30.41</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_italic">Ablations</span></th>
<td class="ltx_td ltx_border_t"></td>
<td class="ltx_td ltx_nopad_r ltx_border_t"></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <math alttext="\vdash" class="ltx_Math" display="inline" id="S5.T3.m1"><semantics><mo>⊢</mo><annotation encoding="application/x-tex">\vdash</annotation></semantics></math> w/o RL-Challenger</th>
<td class="ltx_td ltx_align_center">44.36</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">26.32</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">   <math alttext="\vdash" class="ltx_Math" display="inline" id="S5.T3.m2"><semantics><mo>⊢</mo><annotation encoding="application/x-tex">\vdash</annotation></semantics></math> w/o Rep. Penalty</th>
<td class="ltx_td ltx_align_center">45.76</td>
<td class="ltx_td ltx_nopad_r ltx_align_center">27.56</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   <math alttext="\vdash" class="ltx_Math" display="inline" id="S5.T3.m3"><semantics><mo>⊢</mo><annotation encoding="application/x-tex">\vdash</annotation></semantics></math> w/o Filtering</th>
<td class="ltx_td ltx_align_center ltx_border_bb">47.35</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">24.26</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p">As shown in the table, removing any core components leads to a significant degradation in performance. The largest drop occurs when we disable the Challenger’s reinforcement learning (<span class="ltx_text ltx_font_bold">w/o RL-Challenger</span>), with the Math and General average scores decreasing by 3.7 and 4.1 points, respectively. This result highlighting the importance of our co-evolutionary curriculum generation process.
Similarly, removing the <span class="ltx_text ltx_font_bold">Repetition Penalty</span> also harms performance, indicating that generating a diverse set of questions is crucial for effective Solver training.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p">Finally, disabling the <span class="ltx_text ltx_font_bold">Task Filtering</span> module results in a notable performance drop, particularly on the general-domain average, which falls by over 6 points. As discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S3.SS3" title="3.3 Solver Dataset Construction ‣ 3 Method ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3.3</span></a>, this filtering serves a dual purpose: it calibrates the curriculum’s difficulty and acts as an implicit quality control mechanism by removing questions with low answer consistency. Without this filter, the Solver is trained on a noisy and poorly curated dataset that likely includes ambiguous or ill-posed questions, which harms its ability to learn robustly.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evolution of Question Difficulty and Data Accuracy</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance and data accuracy analysis. The highlighted column represents the <span class="ltx_text ltx_font_italic">true accuracy</span> of the self-generated pseudo-labels for each question set.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5"><span class="ltx_text ltx_font_bold">Performance of Evaluated Model (vs. Ground Truth)</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center ltx_border_t">Base Model</td>
<td class="ltx_td ltx_align_center ltx_border_t">Solver (Iter 1)</td>
<td class="ltx_td ltx_align_center ltx_border_t">Solver (Iter 2)</td>
<td class="ltx_td ltx_align_center ltx_border_t">Solver (Iter 3)</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">Pseudo-Label Acc.</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><math alttext="\mathcal{D}_{\text{Iter 1}}" class="ltx_Math" display="inline" id="S5.T4.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 1</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 1}}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t">48.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">57.0</td>
<td class="ltx_td ltx_align_center ltx_border_t">61.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">79.0%</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><math alttext="\mathcal{D}_{\text{Iter 2}}" class="ltx_Math" display="inline" id="S5.T4.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 2</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 2}}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center">52.5</td>
<td class="ltx_td ltx_align_center">53.0</td>
<td class="ltx_td ltx_align_center">51.5</td>
<td class="ltx_td ltx_align_center">53.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">69.0%</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><math alttext="\mathcal{D}_{\text{Iter 3}}" class="ltx_Math" display="inline" id="S5.T4.m3"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 3</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 3}}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb">44.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">47.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">45.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">50.5</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="background-color:#ECECEC;"><span class="ltx_text" style="background-color:#ECECEC;">63.0%</span></td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p">To understand the co-evolutionary dynamic, we analyzed how the Challenger’s generated questions and their corresponding pseudo-labels change across iterations. We sampled 200 questions from the Challenger’s policy after each of the first three training iterations, creating three distinct test sets: <math alttext="\mathcal{D}_{\text{Iter 1}}" class="ltx_Math" display="inline" id="S5.SS2.p1.m1"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 1</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 1}}</annotation></semantics></math>, <math alttext="\mathcal{D}_{\text{Iter 2}}" class="ltx_Math" display="inline" id="S5.SS2.p1.m2"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 2</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 2}}</annotation></semantics></math>, and <math alttext="\mathcal{D}_{\text{Iter 3}}" class="ltx_Math" display="inline" id="S5.SS2.p1.m3"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>Iter 3</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{Iter 3}}</annotation></semantics></math>. For this analysis, we assumed the external oracle model, GPT-4o, to be a perfect annotator, providing the ground truth answers for all generated questions.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p">The evaluation was conducted as follows: the performance of our internal models was measured against these GPT-4o ground truth answers. The score reported for GPT-4o itself, however, reflects the <span class="ltx_text ltx_font_bold">true accuracy of our self-generated pseudo-labels</span> by comparing the pseudo label against the ground truth from the oracle (GPT-4o). The results on the filtered dataset are summarized in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.T4" title="Table 4 ‣ 5.2 Evolution of Question Difficulty and Data Accuracy ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">4</span></a>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p3">
<p class="ltx_p">This analysis reveals a multi-faceted dynamic. The first finding is that the questions generated by the Challenger become <span class="ltx_text ltx_font_bold">progressively more difficult</span>. This is directly evidenced by evaluating a fixed model against the evolving question sets. For instance, the performance of the static Solver (Iter 1), when measured against the consistent GPT-4o ground truth, drops from 59.0% on Iteration 1 questions to 47.0% on Iteration 3 questions. This confirms that the Challenger is successfully increasing the intrinsic difficulty of its curriculum.
The second finding, revealed by the highlighted column, pertains to the <span class="ltx_text ltx_font_bold">true accuracy of the self-generated dataset</span>. Unfortunately, while the accuracy of the pseudo-labels is initially high at 79.0%, it systematically drops to 63.0% by the third iteration. This trend indicates that as the system generates more difficult problems, the Solver’s majority vote becomes a less reliable source for ground truth. This decline in data quality is a critical trade-off and a potential bottleneck for the framework’s ultimate performance.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p">Finally, despite this drop in absolute label accuracy, the framework’s internal reward mechanism functions precisely as designed. The scores on the table’s diagonal show how each Solver performs on questions from its contemporary Challenger. The Solver (Iter 2) achieves 51.5% and the Solver (Iter 3) achieves 50.5% on their respective question sets. This demonstrates that the Challenger successfully calibrates the question difficulty to match the Solver’s evolving capabilities, consistently targeting the 50% success rate that our reward function incentivizes.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Synergy with Supervised Data</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p">To analyze the utility of our framework in scenarios where a labeled dataset is available, we measure the synergy between <span class="ltx_text ltx_font_italic">R-Zero</span> and traditional supervised fine-tuning using labeled datasets<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/hiyouga/math12k" title="">https://huggingface.co/datasets/hiyouga/math12k</a></span></span></span>. The GRPO settings for this experiment were kept identical to our main experiments.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S5.F3.g1" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/x1.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance of <span class="ltx_text ltx_font_italic">R-Zero</span> when combined with supervised fine-tuning. The dashed line represents the baseline of fine-tuning the base model on labelled data alone, showing that our iterative method provides a better initialization.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p">We first establish a supervised baseline by fine-tuning the base model directly on the labeled data. For this process, we employ GRPO, an approach similar to Zero-RL&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zeng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib47" title="">2025</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p3">
<p class="ltx_p">We then apply our <span class="ltx_text ltx_font_italic">R-Zero</span> framework, where at the end of each co-evolutionary iteration, the resulting checkpoint is also fine-tuned on the same labeled dataset. The results show that our method provides significant additional gains. As highlighted in Figure&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.F3" title="Figure 3 ‣ 5.3 Synergy with Supervised Data ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">3</span></a>, this represents a gain of <span class="ltx_text ltx_font_bold">+2.35 points</span> over the direct training baseline.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p">This finding confirms that <span class="ltx_text ltx_font_italic">R-Zero</span> is not redundant with labeled data; instead, it acts as a powerful performance amplifier. The co-evolutionary process enables the model to better leverage the supervised information and achieve performance levels unattainable by standard fine-tuning alone.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Iteration Scaling</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p">Previous results demonstrate that &nbsp;<span class="ltx_text ltx_font_italic">R-Zero</span> generally enhance the Solver’s capabilities across iterations. A closer inspection, however, reveals that the improvement is not consistent, with performance on certain challenging benchmarks degrading in later iterations. This raises a critical question about the long-term stability of our self-improvement loop: <span class="ltx_text ltx_font_italic">what are the limits of this process, and what causes this eventual performance degradation?</span> In this section, we conduct a dedicated analysis to investigate these iteration scaling dynamics, aiming to diagnose the underlying cause of this instability.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1 </span>The Inevitability of Collapse: An Empirical Analysis</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_figure ltx_align_floatright" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S5.F4.g1" src="./R-Zero_ Self-Evolving Reasoning LLM from Zero Data_files/x2.png" width="830">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Math performance across different iteration times and model scales. The star markers indicate the peak performance for each model size.</figcaption>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS1.p1">
<p class="ltx_p">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.F4" title="Figure 4 ‣ 5.4.1 The Inevitability of Collapse: An Empirical Analysis ‣ 5.4 Iteration Scaling ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">4</span></a>, our framework initially delivers on its promise, with models of all sizes showing significant performance improvements in the early stages of co-evolution. Unfortunately, this virtuous cycle does not continue indefinitely. After multiple iterations, we observe a consistent and concerning trend of performance degradation across all models. Intriguingly, we found a direct correlation between model scale and resilience to this collapse: the larger the model, the later the onset of performance degradation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS1.p2">
<p class="ltx_p">For instance, the smallest 0.6B model reaches its peak performance as early as the first iteration (Iter 1), after which its capabilities begin to decline. In contrast, the largest 4B model sustains its upward trajectory for three full iterations, only experiencing a sharp drop at Iter 4. This pattern strongly suggests that while larger model capacity can delay the negative effects, it does not prevent them. This eventual collapse points to an inherent instability or limitation within our current self-improvement framework, highlighting a critical area for future investigation.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2 </span>Beyond Label Noise: Unpacking the Roots of Instability</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table ltx_align_floatright" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Accuracy of self-generated pseudo-labels (%), labeled by Gemini. Shaded and bolded values indicate the best checkpoint for each model size.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Iteration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Model Size</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">0.6B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">1.7B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">4B</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Iter 1</th>
<td class="ltx_td ltx_align_center ltx_border_t" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">69.4</td>
<td class="ltx_td ltx_align_center ltx_border_t">71.0</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Iter 2</th>
<td class="ltx_td ltx_align_center">53.4</td>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">55.2</span></td>
<td class="ltx_td ltx_align_center">56.2</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Iter 3</th>
<td class="ltx_td ltx_align_center">50.8</td>
<td class="ltx_td ltx_align_center">52.2</td>
<td class="ltx_td ltx_align_center" style="background-color:#ECECEC;"><span class="ltx_text ltx_font_bold" style="background-color:#ECECEC;">48.8</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Iter 4</th>
<td class="ltx_td ltx_align_center ltx_border_bb">44.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb">45.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb">42.2</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS2.p1">
<p class="ltx_p">The most immediate hypothesis for this performance collapse is the degradation of pseudo-label quality, a potential failure mode of the self-correction mechanism we discussed in Section&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.SS2" title="5.2 Evolution of Question Difficulty and Data Accuracy ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">5.2</span></a>. As the Challenger generates increasingly difficult problems, it is plausible that the Solver’s majority vote becomes a less reliable source for ground truth, resulting in a noisy training signal that could ultimately harm performance. To empirically test the extent to which this is the primary cause, we sampled 500 questions from a later training iteration to conduct a more granular investigation into the relationship between pseudo-label fidelity and the observed performance drop.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS2.p2">
<p class="ltx_p">Although the degradation of pseudo-label accuracy is a consistent trend across iterations, our analysis suggests this is not the primary, nor even the sole, driver of the eventual performance collapse. Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.T5" title="Table 5 ‣ 5.4.2 Beyond Label Noise: Unpacking the Roots of Instability ‣ 5.4 Iteration Scaling ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">5</span></a> presents the pseudo-label data quality for each model at the onset of its performance collapse. Intriguingly, there appears to be no universal accuracy threshold that triggers this degradation. For instance, the 0.6B model begins to decline when data accuracy is still as high as 70.6% (Iter 1), whereas the 4B model tolerates an accuracy as low as 48.8% (Iter 3) before its performance drops.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.SSS2.p3">
<p class="ltx_p">This suggests that the absolute percentage of label noise is not the sole determinant of instability. Another potential, and perhaps more fundamental, reason is a form of model collapse that can be introduced when training exclusively on self-synthesized data&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Tan et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib41" title="">2024b</a>; Shumailov et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib37" title="">2024</a>; Dohmatob et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib10" title="">2024b</a>; Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib57" title="">2025b</a>; Seddik et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib30" title="">2024</a>; Dohmatob et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib9" title="">2024a</a>; Briesch et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib4" title="">2023</a>; Zheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib54" title="">2025a</a>)</cite>. A model can enter a degenerative feedback loop, suffering from a loss of diversity or an amplification of its own biases, which presents a significant challenge.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Parameter Sharing
Between Challenger and Solver</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of math performance and pseudo-label accuracy between the standard R-Zero (two-model) and Single-R-Zero (unified model, shared parameters) frameworks across iterations.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold">R-Zero (ours)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span class="ltx_text ltx_font_bold">Single-R-Zero</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span class="ltx_text ltx_font_bold">Iteration</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Performance</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Pseudo-label Acc (%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Performance</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Pseudo-label Acc (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Iter 1</th>
<td class="ltx_td ltx_align_center ltx_border_t">48.06</td>
<td class="ltx_td ltx_align_center ltx_border_t">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">47.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">63.4</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Iter 2</th>
<td class="ltx_td ltx_align_center">48.44</td>
<td class="ltx_td ltx_align_center">56.2</td>
<td class="ltx_td ltx_align_center">46.95</td>
<td class="ltx_td ltx_align_center">46.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Iter 3</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">49.07</span></td>
<td class="ltx_td ltx_align_center">48.8</td>
<td class="ltx_td ltx_align_center">45.57</td>
<td class="ltx_td ltx_align_center">32.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Iter 4</th>
<td class="ltx_td ltx_align_center ltx_border_bb">46.52</td>
<td class="ltx_td ltx_align_center ltx_border_bb">42.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb">43.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb">33.8</td>
</tr>
</tbody>
</table>
</figure><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S5.SS5.p1">
<p class="ltx_p">To investigate whether the separation of the Challenger and Solver into two independent models is a necessary component for the success of <span class="ltx_text ltx_font_italic">R-Zero</span>, we conduct an ablation study using a unified model with shared parameters. In this configuration (Single-R-Zero), a single model is tasked with performing both roles, i.e., generating a challenging curriculum and subsequently learning from it.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p">The results, presented in Table&nbsp;<a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#S5.T6" title="Table 6 ‣ 5.5 Parameter Sharing Between Challenger and Solver ‣ 5 Analysis ‣ R-Zero: Self-Evolving Reasoning LLM from Zero Data"><span class="ltx_text ltx_ref_tag">6</span></a>, clearly indicate that separating the Challenger and Solver into two independent models is crucial for both performance and stability. We observe two key findings. First, our standard two-model R-Zero framework not only achieves a higher peak performance (49.07) but also sustains improvement for more iterations, with its collapse occurring after the third iteration. In contrast, the unified Single-R-Zero model’s performance peaks after the very first iteration and degrades immediately thereafter.
Second, the Single-R-Zero model, where the agent must generate and solve its own problems, produces pseudo-labels of significantly lower accuracy at every stage. For example, in the first iteration, its pseudo-label accuracy is already substantially lower than the R-Zero’s (63.4% vs. 71.0%). We hypothesize that this is because having the problem-setter and solver originate from the same model leads to a form of overconfidence that comes from internal bias.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Label-Free Reinforcement Learning</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p">A significant trend in recent research is Label-Free Reinforcement Learning, which aims to improve LLM reasoning without human-annotated data. Many such methods use the model’s own outputs as a reward signal. This includes leveraging sequence-level confidence&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib22" title="">2025a</a>; Prabhudesai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib29" title="">2025</a>)</cite>, the consistency of answers derived from varied reasoning paths&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib49" title="">2025a</a>; Zuo et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib59" title="">2025</a>; Zhang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib50" title="">2025b</a>)</cite>, minimizing the output entropy&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Agarwal et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib1" title="">2025</a>; Cheng et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib5" title="">2025</a>)</cite>, or even random&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib32" title="">2025</a>)</cite> or negative reward&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhu et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib58" title="">2025</a>)</cite>. These signals are often used within self-training loops where models fine-tune on their own most plausible solutions&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Shafayat et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib31" title="">2025</a>; Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib52" title="">2025b</a>)</cite>. While these methods all rely on a pre-existing set of unlabeled problems, <span class="ltx_text ltx_font_italic">R-Zero</span> removes the need for any seed dataset.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Self-Play in Large Language Models</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p">The paradigm of self-play, where models take on dual roles to create a self-improvement loop, has recently been adapted to improve language models without human data. This approach has been particularly fruitful in verifiable domains like code generation, where a “Coder” agent’s program is verified by a “Tester” agent’s unit tests&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Lin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib26" title="">2025</a>; Wang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib43" title="">2025a</a>; Pourcel et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib28" title="">2025</a>)</cite>. More advanced frameworks push autonomy further by learning to generate the problems themselves, creating an adaptive curriculum from a small seed of examples or from scratch&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Zhao et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib51" title="">2025a</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib25" title="">2025c</a>; Zhou et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib56" title="">2025a</a>; Fang et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib13" title="">2025</a>)</cite>. Our work distinguishes itself by extending this paradigm to general reasoning domains that lack such verifiable environments, instead learning from a reward signal derived from the model’s own internal consistency.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Reinforcement Learning with Verifiable Rewards (RLVR)</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p">Reinforcement Learning with Verifiable Rewards (RLVR) has been widely adopted as a versatile paradigm for enhancing LLMs across a multitude of tasks. Its effectiveness is demonstrated in diverse applications such as relation extraction <cite class="ltx_cite ltx_citemacro_citep">(Dai et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib7" title="">2025</a>)</cite>, interactive GUI navigation <cite class="ltx_cite ltx_citemacro_citep">(Shi et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib35" title="">2025b</a>)</cite> and search-engine utilization <cite class="ltx_cite ltx_citemacro_citep">(Jin et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib19" title="">2025</a>)</cite>. While early implementations relied on rule-based verifiers, recent work has begun to explore more sophisticated, model-based verifiers&nbsp;<cite class="ltx_cite ltx_citemacro_citep">(Ma et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib27" title="">2025</a>; Li et&nbsp;al., <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib24" title="">2025b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2508.05004v2#bib.bib23" title="">2024</a>)</cite>.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future Work</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p">In this paper, we introduced <span class="ltx_text ltx_font_italic">R-Zero</span>, a fully autonomous self-evolving framework that overcomes data dependency by having a Challenger and Solver co-evolve to create a self-generating curriculum. Our experiments demonstrate that <span class="ltx_text ltx_font_italic">R-Zero</span> significantly improves LLM’s reasoning capability on multiple domains. Future work could further focus on improving efficiency, exploring more robust labeling techniques, and expanding <span class="ltx_text ltx_font_italic">R-Zero</span> to new domains. It is crucial to note, however, that the core mechanism of <span class="ltx_text ltx_font_italic">R-Zero</span> is currently suited for domains where correctness can be objectively determined. Extending this self-evolutionary paradigm to open-ended generative tasks, such as creative writing or dialogue, where evaluation is subjective, remains a significant hurdle for future research. We believe <span class="ltx_text ltx_font_italic">R-Zero</span> is a significant step towards creating truly self-evolving LLMs.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of entropy minimization in llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.15134, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bae et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sanghwan Bae, Jiwoo Hong, Min&nbsp;Young Lee, Hanbyul Kim, JeongYeon Nam, et&nbsp;al.

</span>
<span class="ltx_bibblock">Online difficulty filtering for reasoning oriented reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2504.03380, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bercovich et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, et&nbsp;al.

</span>
<span class="ltx_bibblock">Llama-nemotron: Efficient reasoning models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.00949, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briesch et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Martin Briesch, Dominik Sobania, and Franz Rothlauf.

</span>
<span class="ltx_bibblock">Large language models suffer from their own output: An analysis of the self-consuming training loop.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2311.16822, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo&nbsp;Dai, Wayne&nbsp;Xin Zhao, et&nbsp;al.

</span>
<span class="ltx_bibblock">Reasoning with exploration: An entropy perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.14758, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et&nbsp;al. (2021)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mo&nbsp;Bavarian, Mark Chen, Heewoo Jun, et&nbsp;al.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2110.14168, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Runpeng Dai, Tong Zheng, Run Yang, and Hongtu Zhu.

</span>
<span class="ltx_bibblock">R1-re: Cross-domain relationship extraction with rlvr.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2507.04642, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2501.12948, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dohmatob et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe.

</span>
<span class="ltx_bibblock">Strong model collapse.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2410.04840, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dohmatob et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Elvis Dohmatob, Yunzhen Feng, Pu&nbsp;Yang, François Charton, and Julia Kempe.

</span>
<span class="ltx_bibblock">A tale of tails: Model collapse as a change of scaling laws.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et&nbsp;al.

</span>
<span class="ltx_bibblock">Supergpqa: Scaling llm evaluation across 285 graduate disciplines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2502.14739, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, et&nbsp;al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2407.21783, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, et&nbsp;al.

</span>
<span class="ltx_bibblock">Serl: Self-play reinforcement learning for large language models with limited data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.20347, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen&nbsp;Leng Thai, et&nbsp;al.

</span>
<span class="ltx_bibblock">Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proc. of ICLR</em>, 2021a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et&nbsp;al. (2021b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, et&nbsp;al.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the math dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2103.03874, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huan et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, et&nbsp;al.

</span>
<span class="ltx_bibblock">Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning.

</span>
<span class="ltx_bibblock">2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang.

</span>
<span class="ltx_bibblock">Efficient test-time scaling via self-calibration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2503.00031, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han.

</span>
<span class="ltx_bibblock">Search-r1: Training llms to reason and leverage search engines with reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2503.09516, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lambert et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Nathan Lambert, Jacob&nbsp;Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et&nbsp;al.

</span>
<span class="ltx_bibblock">Tülu 3: Pushing frontiers in open language model post-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2411.15124, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et&nbsp;al. (2022)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay&nbsp;V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.

</span>
<span class="ltx_bibblock">Solving quantitative reasoning problems with language models.

</span>
<span class="ltx_bibblock">In Sanmi Koyejo, S.&nbsp;Mohamed, A.&nbsp;Agarwal, Danielle Belgrave, K.&nbsp;Cho, and A.&nbsp;Oh (eds.), <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan&nbsp;V. Oseledets.

</span>
<span class="ltx_bibblock">Confidence is all you need: Few-shot rl fine-tuning of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.06395, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ruosen Li, Ziming Luo, and Xinya Du.

</span>
<span class="ltx_bibblock">Fg-prm: Fine-grained hallucination detection and mitigation in language model mathematical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2410.06304, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo&nbsp;Yeon Sung, and Jordan&nbsp;Lee Boyd-Graber.

</span>
<span class="ltx_bibblock">Semantically-aware rewards for open-ended r1 training in free-form generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.15068, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et&nbsp;al. (2025c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan&nbsp;Lee Boyd-Graber.

</span>
<span class="ltx_bibblock">Videohallu: Evaluating and mitigating multi-modal hallucinations on synthetic video understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.01481, 2025c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zi&nbsp;Lin, Sheng Shen, Jingbo Shang, Jason Weston, and Yixin Nie.

</span>
<span class="ltx_bibblock">Learning to solve and verify: A self-play framework for code and test generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2502.14948, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xueguang Ma, Qian Liu, Dongfu Jiang, Ge&nbsp;Zhang, Zejun Ma, et&nbsp;al.

</span>
<span class="ltx_bibblock">General-reasoner: Advancing llm reasoning across all domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.14652, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pourcel et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Julien Pourcel, Cédric Colas, and Pierre-Yves Oudeyer.

</span>
<span class="ltx_bibblock">Self-improving language models for evolutionary program synthesis: A case study on arc-agi.

</span>
<span class="ltx_bibblock">2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prabhudesai et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Maximizing confidence alone improves reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.22660, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seddik et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mohamed El&nbsp;Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and Mérouane Debbah.

</span>
<span class="ltx_bibblock">How bad is training on synthetic data? a statistical analysis of language model collapse.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2404.05090, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shafayat et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette.

</span>
<span class="ltx_bibblock">Can large reasoning models self-train?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.21444, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Rulin Shao, Shuyue&nbsp;Stella Li, Rui Xin, Scott Geng, Yiping Wang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Spurious rewards: Rethinking training signals in rlvr.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.10947, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, et&nbsp;al.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2402.03300, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao.

</span>
<span class="ltx_bibblock">Efficient reinforcement finetuning via adaptive curriculum learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2504.05520, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment.

</span>
<span class="ltx_bibblock">2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">shoaa kazemi et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mehrangiz shoaa kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, et&nbsp;al.

</span>
<span class="ltx_bibblock">Big-bench extra hard.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Annual Meeting of the Association for Computational Linguistics</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shumailov et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal.

</span>
<span class="ltx_bibblock">Ai models collapse when trained on recursively generated data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature</em>, 631, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yi&nbsp;Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, et&nbsp;al.

</span>
<span class="ltx_bibblock">Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2503.23829, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et&nbsp;al. (2023)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi&nbsp;Tay, Hyung&nbsp;Won Chung, Aakanksha Chowdhery, Quoc Le, Ed&nbsp;Chi, Denny Zhou, and Jason Wei.

</span>
<span class="ltx_bibblock">Challenging BIG-bench tasks and whether chain-of-thought can solve them.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et&nbsp;al. (2024a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu&nbsp;Cheng, and Huan Liu.

</span>
<span class="ltx_bibblock">Large language models for data annotation and synthesis: A survey.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et&nbsp;al. (2024b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu&nbsp;Cheng, and Huan Liu.

</span>
<span class="ltx_bibblock">Large language models for data annotation and synthesis: A survey.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proc. of EMNLP</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, et&nbsp;al.

</span>
<span class="ltx_bibblock">A survey on self-evolution of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2404.14387, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yinjie Wang, Ling Yang, Ye&nbsp;Tian, Ke&nbsp;Shen, and Mengdi Wang.

</span>
<span class="ltx_bibblock">Co-evolving llm coder and unit tester via reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.03136, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2024)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yubo Wang, Xueguang Ma, Ge&nbsp;Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Mmlu-pro: A more robust and challenging multi-task language understanding benchmark.

</span>
<span class="ltx_bibblock">In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub&nbsp;M. Tomczak, and Cheng Zhang (eds.), <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu.

</span>
<span class="ltx_bibblock">Octothinker: Mid-training incentivizes reinforcement learning scaling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.20512, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
An&nbsp;Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et&nbsp;al.

</span>
<span class="ltx_bibblock">Qwen3 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.09388, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, et&nbsp;al.

</span>
<span class="ltx_bibblock">Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2503.18892, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang &amp; Zuo (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Jixiao Zhang and Chunsheng Zuo.

</span>
<span class="ltx_bibblock">Grpo-lead: A difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2504.09696, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Kongcheng Zhang, Qi&nbsp;Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, et&nbsp;al.

</span>
<span class="ltx_bibblock">Consistent paths lead to truth: Self-rewarding reinforcement learning for llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.08745, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian.

</span>
<span class="ltx_bibblock">Right question is already half the answer: Fully unsupervised llm reasoning incentivization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2504.05812, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Absolute zero: Reinforced self-play reasoning with zero data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.03335, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn&nbsp;Xiaodong Song.

</span>
<span class="ltx_bibblock">Learning to reason without external rewards.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2505.19590, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et&nbsp;al. (2025c)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yulai Zhao, Haolin Liu, Dian Yu, S.&nbsp;Y. Kung, Haitao Mi, and Dong Yu.

</span>
<span class="ltx_bibblock">One token to fool llm-as-a-judge.

</span>
<span class="ltx_bibblock">volume abs/2507.08794, 2025c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Tong Zheng, Lichang Chen, Simeng Han, R&nbsp;Thomas McCoy, and Heng Huang.

</span>
<span class="ltx_bibblock">Learning to reason via mixture-of-thought for logical reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2505.15817</em>, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Easyr1: An efficient, scalable, multi-modality rl training framework.

</span>
<span class="ltx_bibblock">2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2025a)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yifei Zhou, Sergey Levine, Jason&nbsp;E. Weston, Xian Li, and Sainbayar Sukhbaatar.

</span>
<span class="ltx_bibblock">Self-challenging language model agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.01716, 2025a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et&nbsp;al. (2025b)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yujun Zhou, Jiayi Ye, Zipeng Ling, Yufei Han, Yue Huang, Haomin Zhuang, Zhenwen Liang, Kehan Guo, Taicheng Guo, Xiangqi Wang, et&nbsp;al.

</span>
<span class="ltx_bibblock">Dissecting logical reasoning in llms: A fine-grained evaluation and supervision study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2506.04810</em>, 2025b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, et&nbsp;al.

</span>
<span class="ltx_bibblock">The surprising effectiveness of negative reinforcement in llm reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2506.01347, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuo et&nbsp;al. (2025)<button class="back-to-reference-btn" aria-label="Back to the article">↑</button></span>
<span class="ltx_bibblock">
Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li&nbsp;Sheng, Xuekai Zhu, et&nbsp;al.

</span>
<span class="ltx_bibblock">Ttrl: Test-time reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">ArXiv preprint</em>, abs/2504.16084, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experiment Details</h2><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Training Hyperparameter</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p">This section summarizes the most critical algorithmic hyperparameters for the Solver and Challenger training stages. All experiments were conducted using BFloat16 (BF16) mixed precision and FlashAttention 2.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>Solver Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS1.p1">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Global Batch Size</span>: 128</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Learning Rate</span>: <math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="A1.I1.i2.p1.m1"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-6}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Weight Decay</span>: <math alttext="1\times 10^{-2}" class="ltx_Math" display="inline" id="A1.I1.i3.p1.m1"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-2}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">KL Penalty Coefficient</span> (<math alttext="\lambda_{KL}" class="ltx_Math" display="inline" id="A1.I1.i4.p1.m1"><semantics><msub><mi>λ</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{KL}</annotation></semantics></math>): <math alttext="1\times 10^{-2}" class="ltx_Math" display="inline" id="A1.I1.i4.p1.m2"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-2}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Max Steps</span>: 15</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Number of Rollouts</span>: 5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Rollout Temperature</span>: 1.0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Rollout Top-p</span>: 0.99</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>Challenger Training</h4><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS2.p1">
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Global Batch Size</span>: 128</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Learning Rate</span>: <math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="A1.I2.i2.p1.m1"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-6}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Weight Decay</span>: <math alttext="1\times 10^{-2}" class="ltx_Math" display="inline" id="A1.I2.i3.p1.m1"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-2}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">KL Penalty Coefficient</span> (<math alttext="\lambda_{KL}" class="ltx_Math" display="inline" id="A1.I2.i4.p1.m1"><semantics><msub><mi>λ</mi><mrow><mi>K</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi></mrow></msub><annotation encoding="application/x-tex">\lambda_{KL}</annotation></semantics></math>): <math alttext="1\times 10^{-2}" class="ltx_Math" display="inline" id="A1.I2.i4.p1.m2"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-2}</annotation></semantics></math></p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Max Steps</span>: 5</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i6.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Number of Rollouts</span>: 4</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i7.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Rollout Temperature</span>: 1.0</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I2.i8.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Rollout Top-p</span>: 0.99</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Prompt Templates</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p">This section presents the exact prompt templates used for the solver and challenger models.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="95.98" id="A1.SS2.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 95.98" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,95.98) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 0 L 0 95.98 L 600 95.98 L 600 0 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 71.87 L 598.03 71.87 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 80.46)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" style="--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Solver Prompt Template</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="46.28" overflow="visible" style="--fo_width :40.23em;--fo_height:3.15em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 43.59)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">System Message:</span></span>
<span class="ltx_p">Please reason step by step, and put your final answer within \boxed{}.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">User Message:</span></span>
<span class="ltx_p">{<span class="ltx_text ltx_font_italic">problem_statement</span>}</span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">Note: </span><span class="ltx_text ltx_font_typewriter">{<span class="ltx_text ltx_font_italic">problem_statement</span>}</span><span class="ltx_text ltx_font_italic"> is a placeholder for the actual math problem.</span></span>
</span></span></span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<span class="ltx_inline-block"><svg class="ltx_picture" height="227.43" id="A1.SS2.p3.pic1" overflow="visible" version="1.1" viewBox="0 0 600 227.43" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,227.43) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 0 L 0 227.43 L 600 227.43 L 600 0 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 203.32 L 598.03 203.32 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 211.92)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" style="--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Challenger Prompt Template</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="177.73" overflow="visible" style="--fo_width :40.23em;--fo_height:12.84em;--fo_depth :0em;" transform="matrix(1 0 0 -1 0 177.73)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">System Message:</span></span>
<span class="ltx_p">You are an expert competition-math problem setter. FIRST, in your private scratch-pad, think step-by-step to design a brand-new, non-trivial problem. The problem could come from any field of mathematics, including but not limited to algebra, geometry, number theory, combinatorics, prealgebra, probability, statistics, and calculus. Aim for a difficulty such that fewer than 30% of advanced high-school students could solve it. Avoid re-using textbook clichés or famous contest problems.</span>
<span class="ltx_p">THEN, without revealing any of your private thoughts, output <span class="ltx_text ltx_font_bold">exactly</span> the following two blocks:</span>
<span class="ltx_p"><span class="ltx_text ltx_font_typewriter">&lt;question&gt;</span></span>
<span class="ltx_p">{The full problem statement on one or more lines}</span>
<span class="ltx_p"><span class="ltx_text ltx_font_typewriter">&lt;/question&gt;</span></span>
<span class="ltx_p"><span class="ltx_text ltx_font_typewriter">\boxed{final_answer}</span></span>
<span class="ltx_p">Do NOT output anything else—no explanations, no extra markup.</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">User Message:</span></span>
<span class="ltx_p">Generate one new, challenging reasoning question now. Remember to format the output exactly as instructed.</span>
</span></span></span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>GPT-4o Judge Prompt</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p">To programmatically evaluate the correctness of answers on mathematical benchmarks where the final answer can be complex (e.g., simplified expressions), we use GPT-4o as a judge. The exact prompt and configuration used for this evaluation are detailed below.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS3.p2">
<span class="ltx_inline-block"><svg class="ltx_picture" height="142.87" id="A1.SS3.p2.pic1" overflow="visible" version="1.1" viewBox="0 0 600 142.87" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,142.87) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 0 L 0 142.87 L 600 142.87 L 600 0 Z" style="stroke:none"></path></g><g fill="#F2F2FF" fill-opacity="1.0"><path d="M 1.97 1.97 L 1.97 118.76 L 598.03 118.76 L 598.03 1.97 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 127.36)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" style="--fo_width :40.23em;--fo_height:0.69em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 9.61)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:34.98em;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Configuration for GPT-4o as Judge</span></span>
</span></span></span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 16.47)"><foreignobject color="#000000" height="93.17" overflow="visible" style="--fo_width :40.23em;--fo_height:6.54em;--fo_depth :0.19em;" transform="matrix(1 0 0 -1 0 90.48)" width="556.69"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:40.23em;">
<span class="ltx_itemize" id="A1.I3">
<span class="ltx_item" id="A1.I3.i1" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para" id="A1.I3.i1.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Model</span>: <span class="ltx_text ltx_font_typewriter">gpt-4o</span></span>
</span></span>
<span class="ltx_item" id="A1.I3.i2" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span>
<span class="ltx_para ltx_noindent" id="A1.I3.i2.p1">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Temperature</span>: 0.1</span>
</span></span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">System Message:</span></span>
<span class="ltx_quote">
<span class="ltx_p">You are a math answer checker.</span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_bold">User Message Template:</span></span>
<span class="ltx_quote">
<span class="ltx_p">Hi, there is an answer: <span class="ltx_text ltx_font_typewriter">{answer}</span>,</span>
<span class="ltx_p">and the ground truth answer is: <span class="ltx_text ltx_font_typewriter">{response}</span>,</span>
<span class="ltx_p">please check whether the answer is correct or not, and return the **only** Yes or No.</span>
</span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">Note: </span><span class="ltx_text ltx_font_typewriter">{<span class="ltx_text ltx_font_italic">answer</span>}</span><span class="ltx_text ltx_font_italic"> is a placeholder for the model-generated solution, and </span><span class="ltx_text ltx_font_typewriter">{<span class="ltx_text ltx_font_italic">response</span>}</span><span class="ltx_text ltx_font_italic"> is the ground-truth answer from the benchmark.</span></span>
</span></span></span></foreignobject></g></g></svg><button class="sr-only button" style="display: none;">Report issue for preceding element</button></span>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Repetition Penalty Implementation</h3><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.p1">
<p class="ltx_p">To encourage the Challenger to generate a diverse set of questions within each batch, we apply a repetition penalty, <math alttext="r_{\text{rep}}" class="ltx_Math" display="inline" id="A1.SS4.p1.m1"><semantics><msub><mi>r</mi><mtext>rep</mtext></msub><annotation encoding="application/x-tex">r_{\text{rep}}</annotation></semantics></math>. This penalty is designed to disincentivize the model from producing semantically similar questions in the same batch. The implementation is a multi-step process based on clustering questions by their BLEU score similarity.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">1. Pairwise Distance Calculation via BLEU Score</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px1.p1">
<p class="ltx_p">First, we compute a pairwise distance matrix for all questions in a batch. The distance <math alttext="d_{ij}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px1.p1.m1"><semantics><msub><mi>d</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">d_{ij}</annotation></semantics></math> between any two questions, <math alttext="x_{i}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px1.p1.m2"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> and <math alttext="x_{j}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px1.p1.m3"><semantics><msub><mi>x</mi><mi>j</mi></msub><annotation encoding="application/x-tex">x_{j}</annotation></semantics></math>, is defined as one minus their BLEU score:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="A1.Ex9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d_{ij}=1-\text{BLEU}(x_{i},x_{j})" class="ltx_Math" display="block" id="A1.Ex9.m1"><semantics><mrow><msub><mi>d</mi><mrow><mi>i</mi><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mrow><mtext>BLEU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">d_{ij}=1-\text{BLEU}(x_{i},x_{j})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">For this calculation, we specifically use the <span class="ltx_text ltx_font_typewriter">sentence_bleu</span> function from the NLTK library (<span class="ltx_text ltx_font_typewriter">nltk.translate.bleu_score</span>). To ensure numerical stability, especially for shorter questions with limited n-gram overlap, we employ its first smoothing function, <span class="ltx_text ltx_font_typewriter">SmoothingFunction().method1</span>. The questions are tokenized for the BLEU calculation by splitting on whitespace; no further text normalization, such as lowercasing or punctuation removal, is performed.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">2. Agglomerative Clustering</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px2.p1">
<p class="ltx_p">With the pairwise distance matrix computed, we then group similar questions using agglomerative hierarchical clustering. This step is performed using the <span class="ltx_text ltx_font_typewriter">Clustering</span> implementation from the <span class="ltx_text ltx_font_typewriter">scikit-learn</span> library. The clustering algorithm is configured with the following key parameters:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Metric</span>: Set to <span class="ltx_text ltx_font_typewriter">’precomputed’</span>, indicating that we provide our custom BLEU-based distance matrix instead of having the algorithm compute distances.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I4.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Linkage</span>: Set to <span class="ltx_text ltx_font_typewriter">’average’</span>. This method defines the distance between two clusters as the average of the distances between all pairs of questions across the two clusters.</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">3. Final Penalty Calculation</h5><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<div class="ltx_para ltx_noindent" id="A1.SS4.SSS0.Px3.p1">
<p class="ltx_p">Once each question in the batch is assigned to a cluster, the repetition penalty <math alttext="r_{\text{rep}}(x_{i})" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m1"><semantics><mrow><msub><mi>r</mi><mtext>rep</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">r_{\text{rep}}(x_{i})</annotation></semantics></math> for a given question <math alttext="x_{i}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m2"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math> is determined by the relative size of the cluster <math alttext="C_{k}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m3"><semantics><msub><mi>C</mi><mi>k</mi></msub><annotation encoding="application/x-tex">C_{k}</annotation></semantics></math> to which it belongs. The penalty is calculated as:</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
<table class="ltx_equation ltx_eqn_table" id="A1.Ex10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r_{\text{rep}}(x_{i})=\frac{|C_{k}|}{B}" class="ltx_Math" display="block" id="A1.Ex10.m1"><semantics><mrow><mrow><msub><mi>r</mi><mtext>rep</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">|</mo></mrow><mi>B</mi></mfrac></mrow><annotation encoding="application/x-tex">r_{\text{rep}}(x_{i})=\frac{|C_{k}|}{B}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">Here, <math alttext="|C_{k}|" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m4"><semantics><mrow><mo stretchy="false">|</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|C_{k}|</annotation></semantics></math> represents the number of questions in cluster <math alttext="C_{k}" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m5"><semantics><msub><mi>C</mi><mi>k</mi></msub><annotation encoding="application/x-tex">C_{k}</annotation></semantics></math>, and <math alttext="B" class="ltx_Math" display="inline" id="A1.SS4.SSS0.Px3.p1.m6"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is the total number of questions in the batch (i.e., the batch size).</p><button class="sr-only button" style="display: none;">Report issue for preceding element</button>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>

</div>


<button type="button" class="btn btn-primary hover-rp-button" id="openForm">Report Issue</button><div class="modal" id="myForm" role="dialog" aria-labelledby="modal-title"><div class="modal-dialog"><form class="modal-content" id="myFormContent" enctype="multipart/form-data"><div class="modal-header" id="modal-header"><h5 class="modal-title">Report Github Issue</h5><button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><label for="form_title" id="modalTitle">Title:</label><input class="form-control" id="form_title" name="form_title" required="required" placeholder="Enter title"><label for="description" id="selectedTextModalDescription" style="display: none;">Content selection saved. Describe the issue below:</label><label for="description" id="nomralModalDescription">Description:</label><textarea class="form-control" id="description" name="description" required="required" style="height: 80px;" maxlength="500" placeholder="500 characters maximum"></textarea></div><div class="modal-footer d-flex justify-content-end"><button type="submit" class="sr-only button" id="modal-submit-sr">Submit without Github</button><button type="submit" class="btn btn-primary" id="modal-submit">Submit in Github</button></div></form></div></div><button id="small-report-button" type="button" class="btn btn-secondary btn-sm" style="background-color: rgb(179, 27, 27); position: fixed; display: none;">Report Issue for Selection</button><div class="ltx_page_footer">
        <div class="ltx_page_logo">
            Generated by
            <a href="https://math.nist.gov/~BMiller/LaTeXML/" class="ltx_LaTeXML_logo">
                <span style="letter-spacing: -0.2em; margin-right: 0.1em;">
                    L
                    <span style="font-size: 70%; position: relative; bottom: 2.2pt;">A</span>
                    T
                    <span style="position: relative; bottom: -0.4ex;">E</span>
                </span>
                <span class="ltx_font_smallcaps">xml</span>
                <img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==">
            </a>
        </div></div><footer id="footer" class="ltx_document">
        <div class="keyboard-glossary">
            <h2>Instructions for reporting errors</h2>
            <p>We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:</p>
            <ul>
                <li>Click the "Report Issue" button.</li>
                <li>Open a report feedback form via keyboard, use "<strong>Ctrl + ?</strong>".</li>
                <li>Make a text selection and click the "Report Issue for Selection" button near your cursor.</li>
                <li class="sr-only">You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.</li>
            </ul>
            <p>Our team has already identified <a class="ltx_ref" href="https://github.com/arXiv/html_feedback/issues" target="_blank">the following issues</a>. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.</p>
            <p>Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/wiki/Porting-LaTeX-packages-for-LaTeXML" target="_blank">list of packages that need conversion</a>, and welcome <a class="ltx_ref" href="https://github.com/brucemiller/LaTeXML/issues" target="_blank">developer contributions</a>.</p>
        </div>
    </footer></body></html>