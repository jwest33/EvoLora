2025-09-21 05:20:22,651 - loralab.config.config_loader - INFO - Loaded configuration from loralab\config\gemma_config.yaml
2025-09-21 05:20:22,651 - loralab.cli_evolution - INFO - Running in quick test mode
2025-09-21 05:20:22,651 - loralab.utils.output_manager - INFO - Output manager initialized at: lora_runs_gemma\run_20250921_052022
2025-09-21 05:20:22,652 - loralab.evolution.evolutionary_trainer - INFO - Using unsloth backend
2025-09-21 05:20:22,655 - loralab.utils.output_manager - INFO - Config saved to: lora_runs_gemma\run_20250921_052022\config\config.yaml
2025-09-21 05:20:22,769 - loralab.core.unsloth_manager - INFO - Loading Unsloth model: unsloth/gemma-3-270m-it with none quantization
2025-09-21 05:20:22,769 - loralab.core.unsloth_manager - INFO - Using configured dtype: bfloat16
2025-09-21 05:20:27,806 - loralab.core.unsloth_manager - INFO - Applied chat template: gemma3
2025-09-21 05:20:27,808 - loralab.datasets.dataset_loader - INFO - Loading dataset: gsm8k
2025-09-21 05:20:27,808 - loralab.datasets.dataset_loader - INFO - Fetching gsm8k from HuggingFace...
2025-09-21 05:20:31,026 - loralab.datasets.dataset_loader - INFO - Loaded 100 training and 20 evaluation examples
2025-09-21 05:20:31,027 - loralab.evolution.evolutionary_trainer - INFO - 
================================================================================
2025-09-21 05:20:31,027 - loralab.evolution.evolutionary_trainer - INFO - STARTING EVOLUTIONARY OPTIMIZATION
2025-09-21 05:20:31,027 - loralab.evolution.evolutionary_trainer - INFO - ================================================================================
2025-09-21 05:20:31,027 - loralab.core.lora_factory - INFO - Created unique variant: gen0_r4_lr5e-05 (hash: fcba594b)
2025-09-21 05:20:31,028 - loralab.core.lora_factory - INFO - Created unique variant: gen0_r16_lr1e-04 (hash: 598d8d71)
2025-09-21 05:20:32,066 - loralab.core.unsloth_manager - INFO - LoRA variant created with Unsloth optimizations:
2025-09-21 05:20:32,066 - loralab.core.unsloth_manager - INFO -   Rank: 4, Alpha: 4 (multiplier: 1)
2025-09-21 05:20:32,067 - loralab.core.unsloth_manager - INFO -   Trainable params: 368,640 (0.14%)
2025-09-21 05:20:32,067 - loralab.core.unsloth_manager - INFO -   Gradient checkpointing: unsloth
2025-09-21 05:20:32,067 - loralab.core.unsloth_manager - INFO -   RSLoRA: True
2025-09-21 05:20:32,067 - loralab.evolution.evolutionary_trainer - INFO - Train data has 100 total examples
2025-09-21 05:20:32,067 - loralab.evolution.evolutionary_trainer - INFO - Requesting 5 format examples
2025-09-21 05:20:32,067 - loralab.evolution.evolutionary_trainer - INFO - Actually got 5 format examples
2025-09-21 05:20:32,067 - loralab.evolution.evolutionary_trainer - INFO - Pre-training on format with 5 examples for 1 epochs
2025-09-21 05:20:32,067 - loralab.training.grpo_trainer - INFO - Pre-training model on format examples...
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO - Pre-training configuration:
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO -   - Examples: 5
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO -   - Batch size: 1
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO -   - Gradient accumulation: 2
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO -   - Effective batch size: 2
2025-09-21 05:20:32,114 - loralab.training.grpo_trainer - INFO -   - Epochs: 1
2025-09-21 05:20:32,115 - loralab.training.grpo_trainer - INFO -   - Expected training steps: 5
2025-09-21 05:20:32,116 - loralab.training.grpo_trainer - INFO - Created pre-training dataset with 5 examples
2025-09-21 05:20:37,097 - loralab.training.grpo_trainer - INFO - Pre-training completed with final loss: 2.7258
2025-09-21 05:20:37,097 - loralab.training.grpo_trainer - INFO - Starting GRPO training for gen0_r4_lr5e-05
2025-09-21 05:23:56,837 - loralab.config.config_loader - INFO - Loaded configuration from loralab\config\gemma_config.yaml
2025-09-21 05:23:56,837 - loralab.cli_evolution - INFO - Running in quick test mode
2025-09-21 05:23:56,837 - loralab.utils.output_manager - INFO - Output manager initialized at: lora_runs_gemma\run_20250921_052356
2025-09-21 05:23:56,837 - loralab.evolution.evolutionary_trainer - INFO - Using unsloth backend
2025-09-21 05:23:56,840 - loralab.utils.output_manager - INFO - Config saved to: lora_runs_gemma\run_20250921_052356\config\config.yaml
2025-09-21 05:23:56,966 - loralab.core.unsloth_manager - INFO - Loading Unsloth model: unsloth/gemma-3-270m-it with none quantization
2025-09-21 05:23:56,967 - loralab.core.unsloth_manager - INFO - Using configured dtype: bfloat16
2025-09-21 05:24:02,473 - loralab.core.unsloth_manager - INFO - Applied chat template: gemma3
2025-09-21 05:24:02,474 - loralab.datasets.dataset_loader - INFO - Loading dataset: gsm8k
2025-09-21 05:24:02,474 - loralab.datasets.dataset_loader - INFO - Fetching gsm8k from HuggingFace...
2025-09-21 05:24:05,777 - loralab.datasets.dataset_loader - INFO - Loaded 100 training and 20 evaluation examples
2025-09-21 05:24:05,778 - loralab.evolution.evolutionary_trainer - INFO - 
================================================================================
2025-09-21 05:24:05,778 - loralab.evolution.evolutionary_trainer - INFO - STARTING EVOLUTIONARY OPTIMIZATION
2025-09-21 05:24:05,778 - loralab.evolution.evolutionary_trainer - INFO - ================================================================================
2025-09-21 05:24:05,778 - loralab.core.lora_factory - INFO - Created unique variant: gen0_r4_lr5e-05 (hash: fcba594b)
2025-09-21 05:24:05,778 - loralab.core.lora_factory - INFO - Created unique variant: gen0_r16_lr1e-04 (hash: 598d8d71)
2025-09-21 05:24:06,877 - loralab.core.unsloth_manager - INFO - LoRA variant created with Unsloth optimizations:
2025-09-21 05:24:06,878 - loralab.core.unsloth_manager - INFO -   Rank: 4, Alpha: 4 (multiplier: 1)
2025-09-21 05:24:06,879 - loralab.core.unsloth_manager - INFO -   Trainable params: 368,640 (0.14%)
2025-09-21 05:24:06,879 - loralab.core.unsloth_manager - INFO -   Gradient checkpointing: unsloth
2025-09-21 05:24:06,879 - loralab.core.unsloth_manager - INFO -   RSLoRA: True
2025-09-21 05:24:06,879 - loralab.evolution.evolutionary_trainer - INFO - Train data has 100 total examples
2025-09-21 05:24:06,879 - loralab.evolution.evolutionary_trainer - INFO - Requesting 5 format examples
2025-09-21 05:24:06,879 - loralab.evolution.evolutionary_trainer - INFO - Actually got 5 format examples
2025-09-21 05:24:06,879 - loralab.evolution.evolutionary_trainer - INFO - Pre-training on format with 5 examples for 1 epochs
2025-09-21 05:24:06,880 - loralab.training.grpo_trainer - INFO - Pre-training model on format examples...
2025-09-21 05:24:06,923 - loralab.training.grpo_trainer - INFO - Pre-training configuration:
2025-09-21 05:24:06,923 - loralab.training.grpo_trainer - INFO -   - Examples: 5
2025-09-21 05:24:06,923 - loralab.training.grpo_trainer - INFO -   - Batch size: 1
2025-09-21 05:24:06,924 - loralab.training.grpo_trainer - INFO -   - Gradient accumulation: 2
2025-09-21 05:24:06,924 - loralab.training.grpo_trainer - INFO -   - Effective batch size: 2
2025-09-21 05:24:06,924 - loralab.training.grpo_trainer - INFO -   - Epochs: 1
2025-09-21 05:24:06,924 - loralab.training.grpo_trainer - INFO -   - Expected training steps: 5
2025-09-21 05:24:06,926 - loralab.training.grpo_trainer - INFO - Created pre-training dataset with 5 examples
2025-09-21 05:24:12,036 - loralab.training.grpo_trainer - INFO - Pre-training completed with final loss: 2.7258
2025-09-21 05:24:12,037 - loralab.training.grpo_trainer - INFO - Starting GRPO training for gen0_r4_lr5e-05
